"""
Script Workflow API Routes

Active Endpoints:
- StyleA Analysis (streaming + non-streaming)
- Conversation Pipeline (streaming + non-streaming)
- Style Profile Management (save/load/delete)
- Templates
"""

from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import asyncio
import json as json_module
import logging

logger = logging.getLogger("script_workflow")
from modules.script_generator import (
    list_templates,
    get_template,
    SCRIPT_TEMPLATES,
    AdvancedRemakeWorkflow,
    StyleA,
    ConversationStyleAnalyzer
)

from modules.ai_automation import HybridAIClient, AIProvider
from modules.ai_settings_db import get_settings_db
from modules.style_profiles_db import get_style_profiles_db
from modules.websocket_hub import manager as ws_manager

router = APIRouter()


def get_configured_ai_client(model: Optional[str] = None) -> HybridAIClient:
    """
    Create a HybridAIClient with proper configuration from database.
    
    Respects the active_provider setting from AI Settings UI:
    - 'openai': Uses OpenAI API key, base URL, and model
    - 'gemini_api': Uses Gemini API key
    - 'custom': Uses Custom API key/base URL/model (OpenAI-compatible)
    
    Falls back to auto-detection if no active provider is set.
    
    Args:
        model: Optional model override. If provided, uses this model instead of the one from settings.
    """
    settings_db = get_settings_db()
    all_settings = settings_db.get_all_settings()
    
    # active_provider is in app_settings table, not ai_settings
    active_provider = settings_db.get_active_provider()
    
    # Load keys based on active provider
    openai_api_key = None
    openai_base_url = None
    openai_model = None
    gemini_api_key = None
    
    if active_provider == 'custom':
        # Custom API uses OpenAI-compatible SDK, so pass as openai params
        custom_settings = all_settings.get('custom_api', {})
        openai_api_key = custom_settings.get('api_key')
        openai_base_url = custom_settings.get('base_url')
        openai_model = model or custom_settings.get('model')
    elif active_provider == 'openai':
        openai_settings = all_settings.get('openai_api', {})
        openai_api_key = openai_settings.get('api_key')
        openai_base_url = openai_settings.get('base_url')
        openai_model = model or openai_settings.get('model')
    elif active_provider == 'gemini_api':
        gemini_settings = all_settings.get('gemini_api', {})
        gemini_api_key = gemini_settings.get('api_key')
    else:
        # No active provider set â€” load all and let HybridAIClient auto-select
        openai_settings = all_settings.get('openai_api', {})
        openai_api_key = openai_settings.get('api_key')
        openai_base_url = openai_settings.get('base_url')
        openai_model = model or openai_settings.get('model')
        gemini_api_key = all_settings.get('gemini_api', {}).get('api_key')
    
    return HybridAIClient(
        openai_api_key=openai_api_key,
        openai_base_url=openai_base_url,
        openai_model=openai_model,
        gemini_api_key=gemini_api_key
    )



# ===== Request/Response Models =====



# StyleA - 3 Step Analysis Request/Response
class AnalyzeToStyleARequest(BaseModel):
    scripts: List[str]  # 5-20 reference scripts
    model: Optional[str] = None  # ChatGPT model to use (e.g., 'gpt-4o', 'gpt-4o-mini')
    analysis_language: str = "vi"  # Language for analysis prompts and output (vi=Vietnamese, en=English)
    output_language: str = ""  # Target output language - if set and differs from input, scripts will be translated first


class StyleAResponse(BaseModel):
    success: bool
    style_a: Optional[Dict] = None  # The synthesized StyleA profile
    individual_analyses: Optional[List[Dict]] = None  # Analysis of each script
    scripts_analyzed: int = 0
    error: Optional[str] = None



class TemplateInfo(BaseModel):
    id: str
    name: str
    description: str
    structure: List[str]


# ===== API Endpoints =====



@router.get("/templates", response_model=List[TemplateInfo])
async def get_templates():
    """
    Láº¥y danh sÃ¡ch cÃ¡c template cÃ³ sáºµn
    """
    return list_templates()


@router.get("/templates/{template_id}")
async def get_template_by_id(template_id: str):
    """
    Láº¥y chi tiáº¿t má»™t template
    """
    template = get_template(template_id)
    if not template:
        raise HTTPException(
            status_code=404,
            detail=f"Template '{template_id}' khÃ´ng tá»“n táº¡i"
        )
    
    return {
        "id": template_id,
        "name": template["name"],
        "description": template["description"],
        "structure": template["structure"],
        "style": template["style"].to_dict() if hasattr(template["style"], "to_dict") else template["style"]
    }


# ===== Style Profile Management Endpoints =====

class SaveStyleRequest(BaseModel):
    name: str
    profile: Dict[str, Any]
    description: str = ""
    source_scripts_count: int = 1


@router.post("/styles/save")
async def save_style_profile(request: SaveStyleRequest):
    """
    LÆ°u style profile Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng.
    """
    if not request.name or not request.name.strip():
        raise HTTPException(status_code=400, detail="TÃªn style khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng")
    
    if not request.profile:
        raise HTTPException(status_code=400, detail="Profile data khÃ´ng há»£p lá»‡")
    
    try:
        db = get_style_profiles_db()
        profile_id = db.save_profile(
            name=request.name.strip(),
            profile_data=request.profile,
            description=request.description,
            source_scripts_count=request.source_scripts_count
        )
        
        result = {
            "success": True,
            "id": profile_id,
            "message": f"ÄÃ£ lÆ°u style '{request.name}'"
        }
        asyncio.ensure_future(ws_manager.broadcast("styles_updated", {"action": "saved", "id": profile_id}))
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/styles")
async def get_saved_styles():
    """
    Láº¥y danh sÃ¡ch táº¥t cáº£ style profiles Ä‘Ã£ lÆ°u
    """
    try:
        db = get_style_profiles_db()
        profiles = db.get_all_profiles()
        
        return {
            "success": True,
            "profiles": profiles,
            "count": len(profiles)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/styles/{profile_id}")
async def get_style_profile(profile_id: int):
    """
    Láº¥y chi tiáº¿t má»™t style profile
    """
    try:
        db = get_style_profiles_db()
        profile = db.get_profile(profile_id)
        
        if not profile:
            raise HTTPException(status_code=404, detail="Style profile khÃ´ng tá»“n táº¡i")
        
        # Increment use count
        db.increment_use_count(profile_id)
        
        return {
            "success": True,
            "profile": profile
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


class UpdateStyleRequest(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    is_favorite: Optional[bool] = None


@router.put("/styles/{profile_id}")
async def update_style_profile(profile_id: int, request: UpdateStyleRequest):
    """
    Cáº­p nháº­t style profile
    """
    try:
        db = get_style_profiles_db()
        
        updates = {}
        if request.name is not None:
            updates['name'] = request.name
        if request.description is not None:
            updates['description'] = request.description
        if request.is_favorite is not None:
            updates['is_favorite'] = 1 if request.is_favorite else 0
        
        if not updates:
            raise HTTPException(status_code=400, detail="KhÃ´ng cÃ³ thÃ´ng tin cáº­p nháº­t")
        
        success = db.update_profile(profile_id, updates)
        
        if not success:
            raise HTTPException(status_code=404, detail="Style profile khÃ´ng tá»“n táº¡i")
        
        asyncio.ensure_future(ws_manager.broadcast("styles_updated", {"action": "updated", "id": profile_id}))
        return {
            "success": True,
            "message": "ÄÃ£ cáº­p nháº­t style profile"
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/styles/{profile_id}")
async def delete_style_profile(profile_id: int):
    """
    XÃ³a style profile.
    """
    try:
        db = get_style_profiles_db()
        success = db.delete_profile(profile_id)
        
        if not success:
            raise HTTPException(status_code=404, detail="Style profile khÃ´ng tá»“n táº¡i")
        
        asyncio.ensure_future(ws_manager.broadcast("styles_updated", {"action": "deleted", "id": profile_id}))
        return {
            "success": True,
            "message": "ÄÃ£ xÃ³a style profile"
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/styles/{profile_id}/favorite")
async def toggle_style_favorite(profile_id: int):
    """
    Toggle tráº¡ng thÃ¡i favorite cá»§a style profile
    """
    try:
        db = get_style_profiles_db()
        success = db.toggle_favorite(profile_id)
        
        if not success:
            raise HTTPException(status_code=404, detail="Style profile khÃ´ng tá»“n táº¡i")
        
        asyncio.ensure_future(ws_manager.broadcast("styles_updated", {"action": "toggled_favorite", "id": profile_id}))
        return {
            "success": True,
            "message": "ÄÃ£ cáº­p nháº­t tráº¡ng thÃ¡i yÃªu thÃ­ch"
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ADVANCED REMAKE - 7 STEP WORKFLOW ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Request/Response Models for Advanced Remake



# Request model for conversation-based pipeline
class AdvancedFullPipelineConversationRequest(BaseModel):
    original_script: str
    target_word_count: int
    source_language: str = ""  # Input language (empty = auto-detect / same as output)
    language: str = "vi"
    dialect: str = "Northern Vietnamese"
    channel_name: str = ""
    country: str = "Vietnam"
    add_quiz: bool = False
    value_type: str = "sell"
    storytelling_style: str = ""  # immersive, documentary, conversational, analytical, narrative
    narrative_voice: str = ""
    custom_narrative_voice: str = ""  # custom cÃ¡ch xÆ°ng hÃ´ ngÃ´i ká»ƒ
    audience_address: str = ""  # cÃ¡ch xÆ°ng hÃ´ khÃ¡n giáº£
    custom_audience_address: str = ""  # custom description
    style_profile: Optional[Dict[str, Any]] = None
    model: Optional[str] = None  # AI model to use (e.g., 'gpt-5.2', 'gpt-4o')
    custom_value: str = ""  # Custom value text for script generation


@router.post("/advanced-remake/full-pipeline-conversation")
async def advanced_full_pipeline_conversation(request: AdvancedFullPipelineConversationRequest):
    """
    ğŸ¯ PIPELINE Má»šI: Cháº¡y toÃ n bá»™ 7 bÆ°á»›c trong 1 CUá»˜C TRÃ’ CHUYá»†N LIÃŠN Tá»¤C
    
    Æ¯u Ä‘iá»ƒm so vá»›i full-pipeline cÅ©:
    - AI nhá»› context xuyÃªn suá»‘t táº¥t cáº£ cÃ¡c bÆ°á»›c
    - Output máº¡ch láº¡c vÃ  liÃªn káº¿t tá»‘t hÆ¡n
    - Giá»ng vÄƒn nháº¥t quÃ¡n tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i
    
    Input: Original script + config
    Output: All analysis results + final coherent script
    """
    import asyncio
    
    if not request.original_script or len(request.original_script.strip()) < 50:
        raise HTTPException(status_code=400, detail="Ká»‹ch báº£n gá»‘c cáº§n Ã­t nháº¥t 50 kÃ½ tá»±")
    
    try:
        # Use selected model from request or fall back to settings
        ai_client = get_configured_ai_client(model=request.model)
        workflow = AdvancedRemakeWorkflow(ai_client)
        
        # Run the conversation-based pipeline
        results = await asyncio.to_thread(
            workflow.full_pipeline_conversation,
            original_script=request.original_script,
            target_word_count=request.target_word_count,
            source_language=request.source_language,
            language=request.language,
            dialect=request.dialect,
            channel_name=request.channel_name,
            country=request.country,
            add_quiz=request.add_quiz,
            value_type=request.value_type,
            storytelling_style=request.storytelling_style,
            narrative_voice=request.narrative_voice,
            custom_narrative_voice=request.custom_narrative_voice,
            audience_address=request.audience_address,
            custom_audience_address=request.custom_audience_address,
            style_profile=request.style_profile,
            custom_value=request.custom_value
        )
        
        # Convert dataclass results to dicts for JSON response
        response_data = {
            "success": True,
            "final_script": results.get("final_script", ""),
            "word_count": results.get("word_count", 0)
        }
        
        # Add optional results if available
        if "original_analysis" in results:
            response_data["original_analysis"] = results["original_analysis"].to_dict() if hasattr(results["original_analysis"], "to_dict") else results["original_analysis"]
        if "structure_analysis" in results:
            response_data["structure_analysis"] = results["structure_analysis"].to_dict() if hasattr(results["structure_analysis"], "to_dict") else results["structure_analysis"]
        if "outline_a" in results:
            response_data["outline_a"] = results["outline_a"].to_dict() if hasattr(results["outline_a"], "to_dict") else results["outline_a"]
        if "draft_sections" in results:
            response_data["draft_sections"] = [d.to_dict() if hasattr(d, "to_dict") else d for d in results["draft_sections"]]
        if "refined_sections" in results:
            response_data["refined_sections"] = [r.to_dict() if hasattr(r, "to_dict") else r for r in results["refined_sections"]]
        
        return response_data
        
    except Exception as e:
        import traceback
        return {
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SCENE SPLITTING - Chia ká»‹ch báº£n thÃ nh scenes
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SplitToScenesRequest(BaseModel):
    script: str
    model: Optional[str] = None
    language: Optional[str] = None  # 'vi', 'en', etc. None = auto-detect
    split_mode: str = "voiceover"  # 'voiceover' (5-8s) or 'footage' (3-5s)


@router.post("/split-to-scenes")
async def split_script_to_scenes(request: SplitToScenesRequest):
    """
    Chia ká»‹ch báº£n thÃ nh cÃ¡c scenes.
    
    Modes:
    - 'voiceover': 5-8 giÃ¢y/scene (máº·c Ä‘á»‹nh, cho voice generation)
    - 'footage': 3-5 giÃ¢y/scene (cho video footage selection)
    
    Sá»­ dá»¥ng thuáº­t toÃ¡n chia thuáº§n (khÃ´ng dÃ¹ng AI) Ä‘á»ƒ Ä‘áº£m báº£o:
    - 100% giá»¯ nguyÃªn ná»™i dung ká»‹ch báº£n
    - Xá»­ lÃ½ tá»©c thá»i, khÃ´ng timeout
    - KhÃ´ng tá»‘n token AI
    """
    if not request.script or len(request.script.strip()) < 50:
        raise HTTPException(status_code=400, detail="Ká»‹ch báº£n cáº§n Ã­t nháº¥t 50 kÃ½ tá»±")

    # Validate split_mode
    split_mode = request.split_mode.lower() if request.split_mode else "voiceover"
    if split_mode not in ["voiceover", "footage"]:
        split_mode = "voiceover"

    try:
        detected_lang = request.language or _detect_language(request.script.strip())
        scenes = _split_script_algorithmically(
            request.script.strip(), 
            language=detected_lang, 
            split_mode=split_mode
        )

        if not scenes:
            raise HTTPException(status_code=500, detail="KhÃ´ng thá»ƒ chia ká»‹ch báº£n. Vui lÃ²ng thá»­ láº¡i.")

        # Verify total word preservation
        original_words = request.script.strip().split()
        scene_words = []
        for s in scenes:
            scene_words.extend(s["content"].split())
        
        total_words = sum(s["word_count"] for s in scenes)
        est_pages = round(total_words / 250, 1)  # ~250 words per A4 page

        mode_label = "voiceover (5-8s)" if split_mode == "voiceover" else "footage (3-5s)"
        
        response_data = {
            "success": True,
            "scenes": scenes,
            "scene_count": len(scenes),
            "detected_language": detected_lang,
            "total_words": total_words,
            "est_pages": est_pages,
            "split_mode": split_mode,
        }

        if split_mode == "footage":
            # Footage mode: no est_duration, will be measured from actual audio
            logger.info(f"[SplitScenes] Mode: {mode_label} | Language: {detected_lang} | Original: {len(original_words)} words -> {len(scenes)} scenes, Preserved: {len(scene_words)} words | ~{est_pages} pages")
        else:
            est_total_duration = sum(s.get("est_duration", 0) for s in scenes)
            response_data["est_total_duration"] = round(est_total_duration, 1)
            logger.info(f"[SplitScenes] Mode: {mode_label} | Language: {detected_lang} | Original: {len(original_words)} words -> {len(scenes)} scenes, Preserved: {len(scene_words)} words | Est: {est_total_duration:.0f}s | ~{est_pages} pages")

        return response_data

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        print(f"Error in split_script_to_scenes: {e}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


# â”€â”€ Clean Scenes Endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class CleanScenesRequest(BaseModel):
    scenes: List[Dict[str, Any]]
    language: Optional[str] = None


@router.post("/clean-scenes")
async def clean_scenes(request: CleanScenesRequest):
    """
    LÃ m sáº¡ch kÃ½ tá»± Ä‘áº·c biá»‡t cho cÃ¡c scenes Ä‘Ã£ cÃ³.
    XÃ³a táº¥t cáº£ dáº¥u ngoáº·c kÃ©p, ngoáº·c Ä‘Æ¡n, ngoáº·c vuÃ´ng, v.v.
    """
    if not request.scenes:
        return {"success": True, "scenes": [], "cleaned_count": 0}

    # Characters to remove (same as Step 2b in split algorithm)
    # Using explicit Unicode code points to avoid encoding issues
    chars_to_remove = [
        '"', "'", '(', ')', '[', ']', '{', '}',
        '\u201C', '\u201D',  # " "
        '\u2018', '\u2019',  # ' '
        '\u00AB', '\u00BB',  # Â« Â»
        '\u2039', '\u203A',  # â€¹ â€º
        '\u201E', '\u201F',  # â€ â€Ÿ
        '\u201A', '\u201B',  # â€š â€›
    ]

    def clean_text(text: str) -> str:
        for char in chars_to_remove:
            text = text.replace(char, '')
        # Clean up double spaces
        import re
        text = re.sub(r' {2,}', ' ', text).strip()
        return text

    language = request.language or 'vi'
    cleaned_scenes = []
    changed_count = 0

    for scene in request.scenes:
        original_content = scene.get('content', '')
        cleaned_content = clean_text(original_content)
        
        # Recount words after cleaning
        word_count = len(cleaned_content.split())
        
        # Estimate duration (same logic as split)
        if language == 'vi':
            wps = 2.0  # Vietnamese ~2 words/sec
        elif language == 'zh':
            wps = 2.5  # Chinese ~2.5 chars/sec
        else:
            wps = 2.5  # Other languages ~2.5 words/sec
        
        est_duration = round(word_count / wps, 1)
        
        # Track if content changed
        if original_content != cleaned_content:
            changed_count += 1
        
        cleaned_scenes.append({
            **scene,
            'content': cleaned_content,
            'word_count': word_count,
            'est_duration': est_duration
        })

    total_words = sum(s['word_count'] for s in cleaned_scenes)
    est_total_duration = sum(s['est_duration'] for s in cleaned_scenes)
    est_pages = round(total_words / 250, 1)

    print(f"[CleanScenes] Cleaned {changed_count}/{len(request.scenes)} scenes | Total: {total_words} words, {est_total_duration:.0f}s")

    return {
        "success": True,
        "scenes": cleaned_scenes,
        "scene_count": len(cleaned_scenes),
        "cleaned_count": changed_count,
        "total_words": total_words,
        "est_total_duration": round(est_total_duration, 1),
        "est_pages": est_pages
    }


# â”€â”€ Language Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _detect_language(text: str) -> str:
    """Detect language from text using character analysis."""
    # Vietnamese diacritics
    vi_chars = set('Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘'
                   'Ã€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä')
    
    sample = text[:1000]
    vi_count = sum(1 for c in sample if c in vi_chars)
    
    if vi_count > len(sample) * 0.02:  # >2% Vietnamese chars
        return 'vi'
    
    # Check for Japanese (hiragana/katakana)
    ja_count = sum(1 for c in sample if '\u3040' <= c <= '\u309f' or '\u30a0' <= c <= '\u30ff')
    if ja_count > len(sample) * 0.05:
        return 'ja'
    
    # Check for Korean (hangul)
    ko_count = sum(1 for c in sample if '\uac00' <= c <= '\ud7af' or '\u1100' <= c <= '\u11ff')
    if ko_count > len(sample) * 0.05:
        return 'ko'
    
    # Check for Chinese (CJK only, after filtering ja/ko)
    cjk_count = sum(1 for c in sample if '\u4e00' <= c <= '\u9fff')
    if cjk_count > len(sample) * 0.1:
        return 'zh'
    
    # Check for Thai
    th_count = sum(1 for c in sample if '\u0e00' <= c <= '\u0e7f')
    if th_count > len(sample) * 0.05:
        return 'th'
    
    # Check for Spanish/French accented chars (rough heuristic)
    es_fr_chars = set('Ã¡Ã©Ã­Ã³ÃºÃ±Ã¼ÃÃ‰ÃÃ“ÃšÃ‘ÃœÂ¿Â¡Ã Ã¢Ã§Ã¨ÃªÃ«Ã®Ã¯Ã´Ã¹Ã»Ã¼Ã¿Å“Ã¦Ã€Ã‚Ã‡ÃˆÃŠÃ‹ÃÃÃ”Ã™Ã›ÃœÅ¸Å’Ã†')
    es_fr_count = sum(1 for c in sample if c in es_fr_chars)
    if es_fr_count > len(sample) * 0.01:
        # Distinguish Spanish vs French by specific chars
        if 'Ã±' in sample.lower() or 'Â¿' in sample or 'Â¡' in sample:
            return 'es'
        if 'Ã§' in sample.lower() or 'Å“' in sample.lower() or 'Ã¦' in sample.lower():
            return 'fr'
    
    return 'en'


def _get_language_params(language: str, split_mode: str = "voiceover") -> dict:
    """
    Get scene splitting parameters based on language, Edge-TTS speech rate, and split mode.
    
    CALIBRATED from actual Edge-TTS voice generation data (130 Vietnamese scenes).
    Edge-TTS speaks significantly slower than natural human speech.
    
    Split Modes:
    - 'voiceover': 5-8 seconds per scene (for voice generation)
    - 'footage': 3-5 seconds per scene (for video footage selection)
    
    Calibrated Edge-TTS speech rates (words per second):
    - Vietnamese: ~2.95 w/s (measured from 130 scenes, avg 7.55s)
    - English:    ~2.8 w/s (Edge-TTS is slower)
    - Chinese:    ~3.0 chars/s
    - Japanese:   ~4.0 mora/s
    - Korean:     ~3.2 syl/s
    - Spanish:    ~4.5 syl/s
    - French:     ~4.2 syl/s
    - Thai:       ~2.8 syl/s
    
    Footage mode (3-5s): Optimized for video footage selection where each clip should
    be short enough to maintain visual interest and match the concept of each scene.
    """
    # Base speech rates (words per second) - same for both modes
    speech_rates = {
        'vi': 2.95,  # Vietnamese: measured ~2.95 w/s
        'en': 2.8,   # English: Edge-TTS is slower
        'zh': 3.0,   # Chinese: chars/s
        'ja': 4.0,   # Japanese: mora/s
        'ko': 3.2,   # Korean: syllables/s
        'es': 4.5,   # Spanish: syllables/s
        'fr': 4.2,   # French: syllables/s
        'th': 2.8,   # Thai: syllables/s
        'de': 3.5,   # German: estimated w/s
        'pt': 4.0,   # Portuguese: estimated w/s
        'ru': 3.5,   # Russian: estimated w/s
    }
    
    if split_mode == "footage":
        # Footage mode: 3-5 seconds per scene
        # Formula: min_words = 3s * speech_rate, max_words = 5s * speech_rate
        params = {
            'vi': {'min_words': 9,  'max_words': 15, 'syl_per_word': 1.0, 'speech_rate': 2.95},   # 3-5s
            'en': {'min_words': 8,  'max_words': 14, 'syl_per_word': 1.0, 'speech_rate': 2.8},    # 3-5s
            'zh': {'min_words': 9,  'max_words': 15, 'syl_per_word': 1.0, 'speech_rate': 3.0},    # 3-5s
            'ja': {'min_words': 12, 'max_words': 20, 'syl_per_word': 2.5, 'speech_rate': 4.0},    # 3-5s
            'ko': {'min_words': 10, 'max_words': 16, 'syl_per_word': 2.0, 'speech_rate': 3.2},    # 3-5s
            'es': {'min_words': 13, 'max_words': 22, 'syl_per_word': 1.9, 'speech_rate': 4.5},    # 3-5s
            'fr': {'min_words': 12, 'max_words': 21, 'syl_per_word': 1.6, 'speech_rate': 4.2},    # 3-5s
            'th': {'min_words': 8,  'max_words': 14, 'syl_per_word': 1.0, 'speech_rate': 2.8},    # 3-5s
            'de': {'min_words': 10, 'max_words': 17, 'syl_per_word': 1.5, 'speech_rate': 3.5},    # 3-5s
            'pt': {'min_words': 12, 'max_words': 20, 'syl_per_word': 1.8, 'speech_rate': 4.0},    # 3-5s
            'ru': {'min_words': 10, 'max_words': 17, 'syl_per_word': 1.5, 'speech_rate': 3.5},    # 3-5s
        }
        default = {'min_words': 9, 'max_words': 15, 'syl_per_word': 1.3, 'speech_rate': 3.0}
    else:
        # Voiceover mode: 5-8 seconds per scene (default)
        # Formula: min_words = 5s * speech_rate, max_words = 8s * speech_rate
        params = {
            'vi': {'min_words': 17, 'max_words': 27, 'syl_per_word': 1.0, 'speech_rate': 2.95},  # 5-8s
            'en': {'min_words': 14, 'max_words': 22, 'syl_per_word': 1.0, 'speech_rate': 2.8},   # 5-8s
            'zh': {'min_words': 15, 'max_words': 24, 'syl_per_word': 1.0, 'speech_rate': 3.0},   # 5-8s
            'ja': {'min_words': 20, 'max_words': 32, 'syl_per_word': 2.5, 'speech_rate': 4.0},   # 5-8s
            'ko': {'min_words': 16, 'max_words': 26, 'syl_per_word': 2.0, 'speech_rate': 3.2},   # 5-8s
            'es': {'min_words': 22, 'max_words': 36, 'syl_per_word': 1.9, 'speech_rate': 4.5},   # 5-8s
            'fr': {'min_words': 21, 'max_words': 34, 'syl_per_word': 1.6, 'speech_rate': 4.2},   # 5-8s
            'th': {'min_words': 14, 'max_words': 22, 'syl_per_word': 1.0, 'speech_rate': 2.8},   # 5-8s
            'de': {'min_words': 17, 'max_words': 28, 'syl_per_word': 1.5, 'speech_rate': 3.5},   # 5-8s
            'pt': {'min_words': 20, 'max_words': 32, 'syl_per_word': 1.8, 'speech_rate': 4.0},   # 5-8s
            'ru': {'min_words': 17, 'max_words': 28, 'syl_per_word': 1.5, 'speech_rate': 3.5},   # 5-8s
        }
        default = {'min_words': 14, 'max_words': 22, 'syl_per_word': 1.3, 'speech_rate': 3.0}
    
    return params.get(language, default)


def _estimate_duration(word_count: int, language: str) -> float:
    """Estimate Edge-TTS speech duration in seconds for a given word count."""
    p = _get_language_params(language)
    # Direct: duration = word_count / speech_rate (words per second)
    return round(word_count / p['speech_rate'], 1)


def _split_script_algorithmically(
    script: str, 
    language: str = 'vi',
    min_words: int = None, 
    max_words: int = None,
    split_mode: str = "voiceover"
) -> List[Dict]:
    """
    Chia ká»‹ch báº£n thÃ nh scenes dá»±a trÃªn thá»i lÆ°á»£ng giá»ng nÃ³i Æ°á»›c tÃ­nh.
    
    Split Modes:
    - 'voiceover': 5-8 seconds per scene (default, for voice generation)
    - 'footage': 3-5 seconds per scene (for video footage selection)
    
    Chiáº¿n lÆ°á»£c v4 (language-aware):
    1. Auto-detect language â†’ set min/max words dá»±a trÃªn speech rate
    2. Normalize whitespace (newline/tab â†’ space)  
    3. TÃ¡ch thÃ nh cÃ¢u táº¡i dáº¥u . ! ? (giá»¯ dáº¥u cÃ¢u)
    4. CÃ¢u dÃ i > max_words â†’ tÃ¡ch táº¡i dáº¥u pháº©y (giá»¯ dáº¥u pháº©y)
    5. Gom cÃ¢u greedy: tÃ­ch luá»¹ cho Ä‘áº¿n khi >= min_words, flush khi thÃªm cÃ¢u vÆ°á»£t max_words
    6. Post-process: gá»™p cÃ¡c scene ngáº¯n liá»n ká»
    
    NguyÃªn táº¯c: KHÃ”NG BAO GIá»œ cáº¯t giá»¯a cÃ¢u. Cho phÃ©p Â±5 tá»« Ä‘á»ƒ giá»¯ nguyÃªn cÃ¢u.
    Äáº£m báº£o 100% giá»¯ nguyÃªn má»i tá»« trong ká»‹ch báº£n gá»‘c.
    """
    import re as re_mod

    # â”€â”€ Step 0: Get language parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lang_params = _get_language_params(language, split_mode)
    if min_words is None:
        min_words = lang_params['min_words']
    if max_words is None:
        max_words = lang_params['max_words']
    
    mode_label = "voiceover (5-8s)" if split_mode == "voiceover" else "footage (3-5s)"
    print(f"[SplitScenes] Mode: {mode_label} | Language: {language} | Target: {min_words}-{max_words} words/scene | Speech rate: {lang_params['speech_rate']} w/s")

    # â”€â”€ Step 1: Normalize all whitespace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    text = script.strip()
    text = re_mod.sub(r'[\r\n\t]+', ' ', text)
    text = re_mod.sub(r' {2,}', ' ', text)
    text = text.strip()

    if not text:
        return []

    # â”€â”€ Step 2: Split into sentences â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Split after sentence-ending punctuation (.!?) followed by whitespace
    # Keep the punctuation attached to preceding text
    parts = re_mod.split(r'(\.{2,}|[.!?])\s+', text)
    
    sentences = []
    i = 0
    while i < len(parts):
        if i + 1 < len(parts) and re_mod.match(r'^(\.{2,}|[.!?])$', parts[i + 1]):
            combined = parts[i].strip() + parts[i + 1]
            if combined.strip():
                sentences.append(combined.strip())
            i += 2
        else:
            if parts[i].strip():
                sentences.append(parts[i].strip())
            i += 1

    if not sentences:
        return []

    # â”€â”€ Step 2b: Remove ALL special quote/bracket characters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TTS reads these characters awkwardly; remove ALL of them early
    # This ensures clean text BEFORE any further processing
    # Using explicit Unicode code points to avoid encoding issues
    _chars_to_remove = [
        '"',        # ASCII double quote (U+0022)
        "'",        # ASCII single quote (U+0027)
        '(',        # Parentheses
        ')',
        '[',        # Square brackets
        ']',
        '{',        # Curly braces
        '}',
        '\u201C',   # Left double quotation mark "
        '\u201D',   # Right double quotation mark "
        '\u2018',   # Left single quotation mark '
        '\u2019',   # Right single quotation mark '
        '\u00AB',   # Left-pointing double angle Â« 
        '\u00BB',   # Right-pointing double angle Â»
        '\u2039',   # Single left-pointing â€¹
        '\u203A',   # Single right-pointing â€º
        '\u201E',   # Double low-9 quotation â€
        '\u201F',   # Double high-reversed-9 â€Ÿ
        '\u201A',   # Single low-9 quotation â€š
        '\u201B',   # Single high-reversed-9 â€›
    ]

    def _clean_special(text: str) -> str:
        """Remove ALL special quote/bracket characters for TTS readability."""
        for char in _chars_to_remove:
            text = text.replace(char, '')
        # Clean up double spaces left behind
        text = re_mod.sub(r' {2,}', ' ', text).strip()
        return text

    sentences = [_clean_special(s) for s in sentences]

    # â”€â”€ Step 3: Split overly long sentences at commas and conjunctions â”€â”€â”€â”€â”€
    # Vietnamese conjunctions and natural break points
    vi_conjunctions = [
        ' vÃ  ', ' nhÆ°ng ', ' hay ', ' hoáº·c ', ' láº¡i ', ' rá»“i ', 
        ' sau Ä‘Ã³ ', ' tiáº¿p theo ', ' nÃªn ', ' vÃ¬ ', ' bá»Ÿi vÃ¬ ',
        ' do Ä‘Ã³ ', ' tuy nhiÃªn ', ' máº·c dÃ¹ ', ' náº¿u ', ' khi ',
        ' thÃ¬ ', ' mÃ  ', ' Ä‘á»ƒ ', ' cÃ²n ', ' cÅ©ng ', ' Ä‘Ã£ ', ' sáº½ ',
        ' cÃ¡i giÃ¡ lÃ  ', ' theo ', ' vá»›i ', ' cho '
    ]
    en_conjunctions = [
        ' and ', ' but ', ' or ', ' then ', ' so ', ' because ',
        ' however ', ' although ', ' if ', ' when ', ' while ',
        ' that ', ' which ', ' where ', ' after ', ' before ',
        ' since ', ' for ', ' yet ', ' thus ', ' therefore '
    ]
    
    def _split_at_conjunctions(text: str) -> List[str]:
        """Split text at natural break points (conjunctions)."""
        # Try Vietnamese first, then English
        conjunctions = vi_conjunctions if language == 'vi' else en_conjunctions
        
        best_split = None
        best_balance = float('inf')  # Lower is better (more balanced split)
        
        for conj in conjunctions:
            if conj in text:
                idx = text.find(conj)
                if idx > 0:
                    left = text[:idx].strip()
                    right = text[idx + len(conj):].strip()
                    if left and right:
                        # Prefer splits that create more balanced parts
                        balance = abs(len(left.split()) - len(right.split()))
                        if balance < best_balance:
                            best_balance = balance
                            best_split = (left, conj.strip(), right)
        
        if best_split:
            left, conj, right = best_split
            # Keep conjunction with first part to maintain meaning
            return [left, right]
        
        return [text]
    
    def _split_long_sentence(sentence: str) -> List[str]:
        """Split sentence > max_words at commas and conjunctions."""
        words = sentence.split()
        if len(words) <= max_words:
            return [sentence]
        
        # First try: Split at commas, keeping comma with preceding part
        comma_parts = re_mod.split(r'(?<=,)\s+', sentence)
        if len(comma_parts) > 1:
            # Greedily group comma-parts into chunks â‰¤ max_words
            result = []
            buf = []
            buf_wc = 0
            for part in comma_parts:
                part = part.strip()
                if not part:
                    continue
                part_wc = len(part.split())
                if buf_wc + part_wc > max_words and buf:
                    result.append(' '.join(buf))
                    buf = []
                    buf_wc = 0
                buf.append(part)
                buf_wc += part_wc
            if buf:
                result.append(' '.join(buf))
        else:
            result = [sentence]
        
        # Second pass: Split any remaining long chunks at conjunctions
        final_result = []
        for chunk in result:
            chunk_wc = len(chunk.split())
            if chunk_wc > max_words:
                # Recursively split at conjunctions
                sub_parts = _split_at_conjunctions(chunk)
                for sub in sub_parts:
                    sub_wc = len(sub.split())
                    if sub_wc > max_words:
                        # Last resort: split at max_words boundary
                        sub_words = sub.split()
                        for i in range(0, len(sub_words), max_words):
                            chunk_slice = ' '.join(sub_words[i:i + max_words])
                            if chunk_slice.strip():
                                final_result.append(chunk_slice.strip())
                    else:
                        final_result.append(sub)
            else:
                final_result.append(chunk)
        
        return final_result if final_result else [sentence]

    segments = []
    for sentence in sentences:
        segments.extend(_split_long_sentence(sentence))

    # â”€â”€ Step 4: Greedy grouping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Accumulate segments. Flush the buffer as a scene when:
    #   - buffer >= min_words AND adding next segment would exceed max_words
    # Allow tolerance of max_words+5 to keep complete sentences together.
    scenes = []
    buf_parts = []
    buf_wc = 0

    for segment in segments:
        seg_wc = len(segment.split())

        if buf_wc == 0:
            buf_parts.append(segment)
            buf_wc = seg_wc
            continue

        combined_wc = buf_wc + seg_wc

        if combined_wc <= max_words:
            # Fits within target â€” keep accumulating
            buf_parts.append(segment)
            buf_wc = combined_wc
        elif buf_wc >= min_words:
            # Buffer already has enough â€” flush it, start fresh
            scenes.append(' '.join(buf_parts))
            buf_parts = [segment]
            buf_wc = seg_wc
        elif combined_wc <= max_words + 5:
            # Slightly over but acceptable to keep sentence intact
            buf_parts.append(segment)
            buf_wc = combined_wc
        else:
            # Buffer too short to flush, but segment too big to add
            # Flush buffer anyway (under min, but better than breaking)
            scenes.append(' '.join(buf_parts))
            buf_parts = [segment]
            buf_wc = seg_wc
    
    if buf_parts:
        scenes.append(' '.join(buf_parts))

    # â”€â”€ Step 4b: Merge consecutive short scenes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if len(scenes) > 1:
        merged = []
        i = 0
        while i < len(scenes):
            current = scenes[i]
            current_wc = len(current.split())
            
            while current_wc < min_words and i + 1 < len(scenes):
                next_scene = scenes[i + 1]
                next_wc = len(next_scene.split())
                
                if current_wc + next_wc <= max_words + 5:
                    current = current + ' ' + next_scene
                    current_wc = current_wc + next_wc
                    i += 1
                else:
                    break
            
            merged.append(current)
            i += 1
        
        scenes = merged

    # â”€â”€ Step 5: Build result with estimated duration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    result = []
    for i, content in enumerate(scenes):
        content = content.strip()
        if content:
            wc = len(content.split())
            scene_data = {
                "scene_id": i + 1,
                "content": content,
                "word_count": wc,
            }
            if split_mode == "footage":
                # Footage mode: no estimated duration, will be measured from actual audio
                scene_data["audio_duration"] = None
            else:
                scene_data["est_duration"] = _estimate_duration(wc, language)
            result.append(scene_data)

    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SCENE CONTEXT ANALYSIS - Character & Setting extraction
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AnalyzeSceneContextRequest(BaseModel):
    script: str
    language: Optional[str] = 'vi'
    model: Optional[str] = None

@router.post("/analyze-scene-context")
async def analyze_scene_context(request: AnalyzeSceneContextRequest):
    """
    Analyze full script to extract consistent characters and settings.
    Returns characters[] and settings[] for use in per-scene prompt generation.
    """
    if not request.script or len(request.script.strip()) < 20:
        raise HTTPException(status_code=400, detail="Script quÃ¡ ngáº¯n Ä‘á»ƒ phÃ¢n tÃ­ch")

    try:
        ai_client = get_configured_ai_client(model=request.model)
        import concurrent.futures

        loop = asyncio.get_event_loop()
        lang_name = {
            'vi': 'Vietnamese', 'en': 'English', 'zh': 'Chinese',
            'ja': 'Japanese', 'ko': 'Korean', 'es': 'Spanish',
            'fr': 'French', 'th': 'Thai', 'pt': 'Portuguese'
        }.get(request.language, 'the given language')

        # Truncate script if too long (use first 3000 + last 1000 chars)
        script_text = request.script.strip()
        if len(script_text) > 4500:
            script_text = script_text[:3000] + "\n...\n" + script_text[-1000:]

        prompt = f"""You are a video production assistant. Analyze this {lang_name} script and extract the main CHARACTERS and SETTINGS for visual consistency across all scenes.

SCRIPT:
{script_text}

Extract:
1. **characters**: List of recurring characters/subjects. For each, provide:
   - id: unique identifier (e.g. "char_1")  
   - name: short name/label
   - visual_description: detailed visual appearance (age, gender, clothing, hair, features) in English for AI image/video generation
   - role: their role (protagonist, narrator, supporting, etc.)

2. **settings**: List of recurring locations/environments. For each, provide:
   - id: unique identifier (e.g. "set_1")
   - name: short name/label
   - visual_description: detailed visual description (lighting, colors, objects, atmosphere) in English for AI image/video generation

IMPORTANT:
- Descriptions must be in English for AI generation tools
- Be specific about visual details
- If the script is narration-style with no clear characters, create a character for the narrator's implied subject
- Return ONLY valid JSON, no other text

Return format:
{{
  "characters": [
    {{"id": "char_1", "name": "...", "visual_description": "...", "role": "protagonist"}}
  ],
  "settings": [
    {{"id": "set_1", "name": "...", "visual_description": "..."}}
  ]
}}"""

        def call_ai(p=prompt):
            return ai_client.generate(p, temperature=0.3)

        with concurrent.futures.ThreadPoolExecutor() as pool:
            result_text = await loop.run_in_executor(pool, call_ai)

        import re as re_mod
        json_match = re_mod.search(r'\{[\s\S]*"characters"[\s\S]*"settings"[\s\S]*\}', result_text)
        if not json_match:
            json_match = re_mod.search(r'\{[\s\S]*\}', result_text)

        if json_match:
            context_data = json_module.loads(json_match.group())
            print(f"[SceneContext] Extracted {len(context_data.get('characters', []))} characters, {len(context_data.get('settings', []))} settings")
            return {
                "success": True,
                "characters": context_data.get("characters", []),
                "settings": context_data.get("settings", []),
            }
        else:
            return {"success": False, "error": "Could not parse AI response", "characters": [], "settings": []}

    except Exception as e:
        print(f"[SceneContext] Error: {e}")
        return {"success": False, "error": str(e), "characters": [], "settings": []}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONCEPT ANALYSIS - Deep script analysis for footage alignment
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AnalyzeConceptRequest(BaseModel):
    script: str
    model: Optional[str] = None

@router.post("/analyze-concept")
async def analyze_concept(request: AnalyzeConceptRequest):
    """
    Analyze the full script to extract holistic concept information:
    theme, visual style, environments, subjects, mood, and footage guidelines.
    This is used to guide keyword generation and footage selection.
    """
    if not request.script or len(request.script.strip()) < 20:
        raise HTTPException(status_code=400, detail="Script qua ngan de phan tich")

    try:
        ai_client = get_configured_ai_client(model=request.model)
        import concurrent.futures

        loop = asyncio.get_event_loop()

        # Truncate script if too long (use first 4000 + last 1000 chars)
        script_text = request.script.strip()
        if len(script_text) > 5500:
            script_text = script_text[:4000] + "\n...\n" + script_text[-1000:]

        prompt = f"""You are a video production director and stock footage search expert. Analyze this script deeply and extract a COMPREHENSIVE CONCEPT PROFILE that will guide stock footage selection.

SCRIPT:
{script_text}

You MUST extract ALL of the following. Pay special attention to items marked [ANCHOR] â€” these will be used as HARD CONSTRAINTS for every keyword search.

1. **theme**: The main topic/subject of the entire script (e.g., "space exploration", "construction timelapse", "ocean wildlife documentary")
2. **visual_style**: Camera style, lighting, and visual approach (e.g., "cinematic slow-motion", "aerial drone", "timelapse photography", "close-up macro")
3. **environments**: List of ALL physical settings/locations mentioned or implied (e.g., ["deep space", "spacecraft interior", "planet surface"])
4. **subjects**: List of ALL people, objects, creatures, or activities (e.g., ["astronaut in spacesuit", "rocket launch", "control room operators"])
5. **mood**: Emotional tone (e.g., "awe-inspiring and epic", "tense and suspenseful")
6. **color_palette**: Dominant colors/tones (e.g., "dark blues with bright highlights")
7. **footage_guidelines**: Specific instructions for footage search to ensure EVERY clip matches the concept
8. **tempo**: Pacing (e.g., "slow and contemplative", "building from slow to intense")

[ANCHOR] KEYWORD CONSTRAINT FIELDS (CRITICAL â€” these control ALL keyword generation):

9. **concept_prefix**: 2-3 word prefix that MUST appear in EVERY search keyword. This ensures visual consistency. Examples: "timelapse construction", "underwater ocean", "aerial city", "close-up nature", "cinematic space". Pick the most defining visual characteristic.
10. **style_modifier**: 1-2 word modifier to append to keywords for style consistency. Examples: "timelapse", "slow motion", "aerial drone", "macro closeup", "4k cinematic"
11. **allowed_environments**: List of 5-10 specific environments that footage can show. Only footage matching these environments should be used. Use simple English words suitable for Pexels/Pixabay search. (e.g., ["construction site", "building foundation", "crane area", "scaffolding", "city skyline"])
12. **allowed_subjects**: List of 8-15 specific subjects/objects/people that can appear in footage. Use simple, concrete English nouns. (e.g., ["crane", "steel beam", "concrete mixer", "construction workers", "excavator", "building frame", "hard hat", "blueprint"])
13. **forbidden_terms**: List of 5-10 terms that MUST NEVER appear in keywords. These are off-topic or wrong-style terms. (e.g., ["cartoon", "animation", "illustration", "office meeting", "classroom", "abstract art"])

CRITICAL RULES:
- concept_prefix and style_modifier must be actual stock footage search terms (test: "would Pexels find results for this?")
- allowed_subjects/environments must be CONCRETE VISUAL things a camera can see, NOT abstract concepts
- forbidden_terms should block common AI mistakes that would produce wrong footage
- Be EXTREMELY specific â€” vague descriptions lead to mismatched footage
- Return ONLY valid JSON, no other text

Return format:
{{
  "theme": "...",
  "visual_style": "...",
  "environments": ["...", "..."],
  "subjects": ["...", "..."],
  "mood": "...",
  "color_palette": "...",
  "footage_guidelines": "...",
  "tempo": "...",
  "concept_prefix": "...",
  "style_modifier": "...",
  "allowed_environments": ["...", "..."],
  "allowed_subjects": ["...", "..."],
  "forbidden_terms": ["...", "..."]
}}"""

        def call_ai(p=prompt):
            return ai_client.generate(p, temperature=0.3)

        with concurrent.futures.ThreadPoolExecutor() as pool:
            result_text = await loop.run_in_executor(pool, call_ai)

        import re as re_mod
        json_match = re_mod.search(r'\{[\s\S]*"theme"[\s\S]*\}', result_text)
        if not json_match:
            json_match = re_mod.search(r'\{[\s\S]*\}', result_text)

        if json_match:
            concept_data = json_module.loads(json_match.group())
            logger.info(f"[ConceptAnalysis] theme='{concept_data.get('theme', '')[:50]}', "
                       f"style='{concept_data.get('visual_style', '')[:50]}', "
                       f"envs={len(concept_data.get('environments', []))}, "
                       f"subjects={len(concept_data.get('subjects', []))}")
            return {
                "success": True,
                "concept": concept_data,
            }
        else:
            return {"success": False, "error": "Could not parse AI response", "concept": None}

    except Exception as e:
        logger.error(f"[ConceptAnalysis] Error: {e}")
        return {"success": False, "error": str(e), "concept": None}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AI KEYWORD & PROMPT GENERATION - Mode-based
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def _validate_keyword_against_concept(keyword: str, concept_analysis: dict) -> str:
    """
    Filter 1: Validate and auto-fix a keyword against the concept anchor.
    - Auto-prepend concept_prefix if missing
    - Strip forbidden terms
    - Log violations for debugging
    """
    if not keyword or not concept_analysis:
        return keyword

    kw_lower = keyword.lower().strip()
    original = keyword
    fixed = False

    # 1. Check forbidden terms -> remove them
    forbidden = concept_analysis.get('forbidden_terms', [])
    if forbidden:
        for term in forbidden:
            term_lower = term.lower()
            if term_lower in kw_lower:
                keyword = keyword.replace(term, '').replace(term.lower(), '').strip()
                keyword = ' '.join(keyword.split())  # Clean double spaces
                kw_lower = keyword.lower()
                fixed = True
                print(f"[KeywordValidation] Removed forbidden term '{term}' from '{original}'")

    # 2. Check concept_prefix -> auto-prepend if missing
    concept_prefix = concept_analysis.get('concept_prefix', '')
    style_modifier = concept_analysis.get('style_modifier', '')

    has_prefix = concept_prefix and concept_prefix.lower() in kw_lower
    has_modifier = style_modifier and style_modifier.lower() in kw_lower

    if concept_prefix and not has_prefix and not has_modifier:
        # Prepend the style_modifier (shorter, better for search) or concept_prefix
        prefix_to_add = style_modifier if style_modifier else concept_prefix
        keyword = f"{prefix_to_add} {keyword}"
        fixed = True
        print(f"[KeywordValidation] Added prefix '{prefix_to_add}' -> '{keyword}'")

    # 3. Log if keyword has no allowed subject/environment match (warning only)
    allowed_subjects = [s.lower() for s in concept_analysis.get('allowed_subjects', [])]
    allowed_envs = [e.lower() for e in concept_analysis.get('allowed_environments', [])]

    if allowed_subjects or allowed_envs:
        has_subject = any(any(word in kw_lower for word in sub.split()) for sub in allowed_subjects)
        has_env = any(any(word in kw_lower for word in env.split()) for env in allowed_envs)

        if not has_subject and not has_env:
            print(f"[KeywordValidation] WARNING: '{keyword}' has no matching subject/env from allowed lists")

    if fixed:
        print(f"[KeywordValidation] '{original}' -> '{keyword}'")

    return keyword.strip()


class SceneKeywordItem(BaseModel):
    scene_id: int
    content: str
    audio_duration: Optional[float] = None  # Actual measured audio duration in seconds

class GenerateSceneKeywordsRequest(BaseModel):
    scenes: List[SceneKeywordItem]
    language: Optional[str] = 'vi'
    model: Optional[str] = None
    # Mode-based generation
    mode: Optional[str] = 'footage'  # footage | concept | storytelling | custom
    generate_image_prompt: Optional[bool] = False
    generate_video_prompt: Optional[bool] = False
    generate_keywords: Optional[bool] = True  # Whether to generate keywords (default True for backward compat)
    consistent_characters: Optional[bool] = False
    consistent_settings: Optional[bool] = False
    # User-provided style and character for prompts
    prompt_style: Optional[str] = None  # e.g. "cinematic photorealism, golden hour, 35mm"
    main_character: Optional[str] = None  # e.g. "28yo Vietnamese man, short black hair, glasses"
    # Context from analyze-scene-context (for consistency)
    scene_context: Optional[Dict[str, Any]] = None
    # Concept analysis from /analyze-concept (for concept-aligned keywords)
    concept_analysis: Optional[Dict[str, Any]] = None
    # Context/environment description for full_sync mode
    context_description: Optional[str] = None  # Environment/setting description for consistency
    # Prompt mode hints
    image_prompt_mode: Optional[str] = None   # reference | scene_builder | concept
    video_prompt_mode: Optional[str] = None   # character_sync | scene_sync | full_sync
    # Direction analysis notes (from video_direction step) for informed prompt generation
    directions: Optional[List[Dict[str, Any]]] = None
    # Sync analysis data (characters, settings, visual_style)
    sync_analysis: Optional[Dict[str, Any]] = None


@router.post("/generate-scene-keywords")
async def generate_scene_keywords(request: GenerateSceneKeywordsRequest):
    """
    Generate keywords and prompts for each scene using AI.
    Supports 4 modes: footage (keyword only), concept (keyword + image),
    storytelling (keyword + image + video), custom (user picks).
    """
    if not request.scenes:
        raise HTTPException(status_code=400, detail="KhÃ´ng cÃ³ scene nÃ o")

    # Determine what to generate based on mode
    gen_image = request.generate_image_prompt
    gen_video = request.generate_video_prompt
    gen_keywords = request.generate_keywords if request.generate_keywords is not None else True
    use_chars = request.consistent_characters
    use_settings = request.consistent_settings

    if request.mode == 'footage':
        gen_image = False
        gen_video = False
        use_chars = False
        use_settings = False
    elif request.mode == 'concept':
        gen_image = True
        gen_video = False
        use_chars = False
        use_settings = False
    elif request.mode == 'storytelling':
        gen_image = True
        gen_video = True
        use_chars = True
        use_settings = True
    # 'custom' uses the flags as-is from request

    async def event_generator():
        try:
            ai_client = get_configured_ai_client(model=request.model)
            import concurrent.futures
            import re as re_mod

            loop = asyncio.get_event_loop()
            lang_name = {
                'vi': 'Vietnamese', 'en': 'English', 'zh': 'Chinese',
                'ja': 'Japanese', 'ko': 'Korean', 'es': 'Spanish',
                'fr': 'French', 'th': 'Thai', 'pt': 'Portuguese'
            }.get(request.language, 'the same language as the scenes')

            total_scenes = len(request.scenes)
            result_map = {}

            mode_label = request.mode or 'footage'
            print(f"[SceneKeywords] Mode={mode_label} | {total_scenes} scenes | image={gen_image} video={gen_video} chars={use_chars} settings={use_settings}")

            BATCH_SIZE = 50  # Max scenes per AI call
            scene_batches = [request.scenes[i:i + BATCH_SIZE] for i in range(0, total_scenes, BATCH_SIZE)]
            num_batches = len(scene_batches)
            batch_label = f'{num_batches} batch' if num_batches > 1 else '1 láº§n'

            yield f"data: {json_module.dumps({'type': 'progress', 'message': f'Keywords: {total_scenes} scenes ({batch_label})', 'current': 0, 'total': total_scenes, 'percentage': 2}, ensure_ascii=False)}\n\n"

            # Build consistency context block
            consistency_block = ""
            if (use_chars or use_settings) and request.scene_context:
                parts = []
                if use_chars and request.scene_context.get('characters'):
                    chars_desc = "\n".join([
                        f"  - {c.get('name', 'Character')}: {c.get('visual_description', '')}"
                        for c in request.scene_context['characters']
                    ])
                    parts.append(f"RECURRING CHARACTERS (use these descriptions consistently):\n{chars_desc}")
                if use_settings and request.scene_context.get('settings'):
                    sets_desc = "\n".join([
                        f"  - {s.get('name', 'Setting')}: {s.get('visual_description', '')}"
                        for s in request.scene_context['settings']
                    ])
                    parts.append(f"RECURRING SETTINGS (use these descriptions consistently):\n{sets_desc}")
                if parts:
                    consistency_block = "\n" + "\n\n".join(parts) + "\n"



            # â”€â”€ BATCHED PROMPT GENERATION â”€â”€
            yield f"data: {json_module.dumps({'type': 'progress', 'message': f'Táº¡o prompts: {total_scenes} scenes ({batch_label})', 'current': 0, 'total': total_scenes, 'percentage': 10}, ensure_ascii=False)}\n\n"

            # Build CONCEPT ANCHOR block
            concept_anchor_block = ""
            has_anchor = False
            if request.concept_analysis:
                ca = request.concept_analysis
                anchor_parts = []
                if ca.get('concept_prefix'):
                    anchor_parts.append(f"CONCEPT PREFIX (MUST appear in EVERY keyword): \"{ca['concept_prefix']}\"")
                if ca.get('style_modifier'):
                    anchor_parts.append(f"STYLE MODIFIER (append to keywords for consistency): \"{ca['style_modifier']}\"")
                if ca.get('allowed_environments') and isinstance(ca['allowed_environments'], list):
                    envs = ', '.join([f'"{e}"' for e in ca['allowed_environments'][:10]])
                    anchor_parts.append(f"ALLOWED ENVIRONMENTS (pick ONLY from this list): [{envs}]")
                if ca.get('allowed_subjects') and isinstance(ca['allowed_subjects'], list):
                    subs = ', '.join([f'"{s}"' for s in ca['allowed_subjects'][:15]])
                    anchor_parts.append(f"ALLOWED SUBJECTS (pick ONLY from this list): [{subs}]")
                if ca.get('forbidden_terms') and isinstance(ca['forbidden_terms'], list):
                    forbidden = ', '.join([f'"{f}"' for f in ca['forbidden_terms'][:10]])
                    anchor_parts.append(f"FORBIDDEN TERMS (NEVER use any of these): [{forbidden}]")
                if ca.get('theme'):
                    anchor_parts.append(f"Theme: {ca['theme']}")
                if ca.get('visual_style'):
                    anchor_parts.append(f"Visual Style: {ca['visual_style']}")
                if ca.get('footage_guidelines'):
                    anchor_parts.append(f"Footage Guidelines: {ca['footage_guidelines']}")

                if anchor_parts:
                    has_anchor = True
                    concept_anchor_block = "\n\nâ•â• CONCEPT ANCHOR (MANDATORY â€” every keyword MUST obey these rules) â•â•\n" + "\n".join(anchor_parts) + "\nâ•â• END CONCEPT ANCHOR â•â•\n"

            concept_prefix_str = request.concept_analysis.get('concept_prefix', '') if request.concept_analysis else ''
            style_modifier_str = request.concept_analysis.get('style_modifier', '') if request.concept_analysis else ''

            # Shared context: direction notes map
            dir_map = {}
            if request.directions and (gen_image or gen_video):
                dir_map = {d.get('scene_id'): d for d in request.directions if d.get('scene_id')}

            # Pre-compute scene metadata for all scenes
            scene_meta = {}
            for scene in request.scenes:
                import math
                if request.mode == 'footage' and scene.audio_duration and scene.audio_duration > 0:
                    keyword_count = max(1, math.ceil(scene.audio_duration / 4))
                    target_clip_duration = round(scene.audio_duration / keyword_count, 1)
                else:
                    keyword_count = 1
                    target_clip_duration = 4.0
                scene_meta[scene.scene_id] = {
                    'keyword_count': keyword_count,
                    'target_clip_duration': target_clip_duration,
                }

            # Build example output block (shared)
            first_scene = request.scenes[0]
            first_kw_count = scene_meta[first_scene.scene_id]['keyword_count']
            example_lines = [f"===SCENE {first_scene.scene_id}==="]
            if gen_keywords:
                if first_kw_count > 1:
                    example_lines.append("KEYWORDS: keyword1 | keyword2")
                else:
                    example_lines.append("KEYWORD: visual keyword describing what camera sees")
            if gen_image:
                example_lines.append("IMAGE_PROMPT: A detailed narrative paragraph for AI image generation following the 7-component structure...")
            if gen_video:
                example_lines.append("VIDEO_PROMPT: A detailed narrative paragraph for AI video generation following the 7-component structure...")
            example_output = "\n".join(example_lines)

            # â”€â”€ 7-COMPONENT UNIVERSAL PROMPTING FRAMEWORK â”€â”€
            prompt_framework_block = ""
            if gen_image or gen_video:
                components_for_image = """
â•â• 7-COMPONENT UNIVERSAL PROMPTING FRAMEWORK â•â•
You MUST generate structured narrative prompts following this exact framework.
Write each prompt as ONE FLOWING PARAGRAPH in natural English â€” NOT as a list of tags or keywords ("tag soup" is forbidden).

The 7 components to weave into each prompt:

1. **SUBJECT** (Who/What): The visual anchor. Include identity, age, physical features, clothing, distinguishing marks.
   - BAD: "a man"
   - GOOD: "a weathered 40-year-old fisherman with a salt-and-pepper beard, wearing a faded yellow raincoat"

2. **CONTEXT** (Where): Environment, time of day, weather, spatial details. Describe light sources so the AI can compute shadows.
   - BAD: "in a room"
   - GOOD: "in a sun-drenched Victorian study, dust motes dancing in shafts of light streaming through tall floor-to-ceiling bay windows"

3. **ACTION** (What happens): For images = pose/state. For video = movement verbs + cause-and-effect interactions.
   - Image: "frozen mid-leap", "sitting pensively"
   - Video: "walking with a heavy limp", "water splashing violently as she dives in"

4. **CINEMATOGRAPHY** (How): Shot type + lens + camera movement.
   - Shot types: ECU (extreme close-up), CU (close-up), MS (medium shot), WS (wide shot)
   - Lenses: "35mm natural", "85mm portrait bokeh", "macro 100mm", "fisheye wide-angle"
   - Camera (video): "slow dolly in", "orbit/arc shot", "handheld shaky", "crane up reveal", "tracking shot"

5. **AESTHETICS** (Feel): Art style, lighting technique, color palette, mood/emotion.
   - Styles: "photorealistic", "cinematic", "3D render (Unreal Engine 5)", "oil painting", "anime (Studio Ghibli)"
   - Lighting: "golden hour warm", "blue hour cold", "noir high-contrast", "volumetric god rays", "softbox studio"
   - Colors: describe dominant palette tones

6. **AUDIO** (Soundscape â€” VIDEO PROMPTS ONLY): Dialogue (in quotes), SFX quality, ambient environment sounds.
   - BAD: "has audio"
   - GOOD: "Audio: crisp crunch of dry leaves underfoot, labored breathing, and the distant plaintive cry of a crow"

7. **CONSTRAINTS** (Guardrails): Aspect ratio, resolution, negative prompt (what to avoid).
   - "No text overlays, no blurry backgrounds, no distorted fingers, 16:9 cinematic"

â•â• CRITICAL RULES FOR PROMPTS â•â•
- Write as ONE FLOWING NARRATIVE PARAGRAPH per prompt â€” NOT bullet points or tag lists
- Use NATURAL LANGUAGE with cause-and-effect reasoning ("because", "as", "while")
- All prompts MUST be in ENGLISH for AI generation tools
- Each scene prompt must be UNIQUE and tell a different visual story
- Maintain CHARACTER CONSISTENCY across all scenes (same descriptions for recurring characters)
- Maintain SETTING CONSISTENCY (same environment details when scenes share locations)
- **WORD COUNT LIMITS (STRICTLY ENFORCED)**:
  - IMAGE_PROMPT: 40-60 words maximum. Be precise and dense â€” every word must add visual information. Cut filler words ruthlessly.
  - VIDEO_PROMPT: 80-120 words maximum. Include motion, camera, audio â€” but stay concise. No verbose padding.
  - These limits ensure compatibility with Midjourney (60-word sweet spot), DALL-E 3, Flux (77-token CLIP limit), VEO3, and Runway (1000-char limit).
"""
                prompt_framework_block = components_for_image

                if request.main_character:
                    prompt_framework_block += f"""
â•â• MAIN CHARACTER (must appear consistently in ALL scenes) â•â•
{request.main_character}
Use this EXACT description every time the main character appears. Do NOT change their appearance across scenes.
"""
                if request.prompt_style:
                    prompt_framework_block += f"""
â•â• VISUAL STYLE DIRECTION (apply to ALL prompts) â•â•
{request.prompt_style}
Weave this style consistently into the Aesthetics and Cinematography components of every prompt.
"""
                if request.context_description:
                    prompt_framework_block += f"""
â•â• ENVIRONMENT/CONTEXT (maintain consistency across ALL scenes) â•â•
{request.context_description}
Use this EXACT environment description when the scene takes place in this setting. Maintain spatial continuity, lighting, and atmosphere.
"""
                if request.sync_analysis:
                    sa = request.sync_analysis
                    sync_parts = []
                    if sa.get('characters'):
                        chars = "\n".join([f"  - {c.get('name', '')}: {c.get('visual_description', c.get('description', ''))}" for c in sa['characters'][:5]])
                        sync_parts.append(f"SYNC CHARACTERS (use these descriptions consistently):\n{chars}")
                    if sa.get('settings'):
                        sets = "\n".join([f"  - {s.get('name', '')}: {s.get('visual_description', s.get('description', ''))}" for s in sa['settings'][:5]])
                        sync_parts.append(f"SYNC SETTINGS (use these descriptions consistently):\n{sets}")
                    if sa.get('visual_style'):
                        sync_parts.append(f"SYNC VISUAL STYLE (apply to ALL prompts): {sa['visual_style']}")
                    if sync_parts:
                        prompt_framework_block += "\nâ•â• SYNC ANALYSIS (follow these constraints for consistency) â•â•\n" + "\n".join(sync_parts) + "\n"

                if request.video_prompt_mode == 'character_sync':
                    prompt_framework_block += """
â•â• MODE: CHARACTER SYNC â•â•
PRIORITY: Character identity consistency is the #1 concern. Repeat full character description (Identity Layer) in EVERY prompt.
Ensure the same person is recognizable across all scenes â€” same face, hair, body type, clothing.
"""
                elif request.video_prompt_mode == 'scene_sync':
                    prompt_framework_block += """
â•â• MODE: STYLE SYNC â•â•
PRIORITY: Visual style and aesthetic consistency is the #1 concern.
Maintain identical color grading, lighting style, camera language, and art direction across ALL scenes.
"""
                elif request.video_prompt_mode == 'full_sync':
                    prompt_framework_block += """
â•â• MODE: FULL SYNC (Character + Style + Context) â•â•
PRIORITY: Triple consistency â€” Character identity + Visual style + Environment must ALL remain consistent.
1. Repeat full character description (Identity Layer) in EVERY prompt
2. Maintain identical aesthetics (color grade, lighting, lens) across ALL scenes
3. Keep environment/spatial details consistent when scenes share locations
"""
                if gen_image and not gen_video:
                    prompt_framework_block += """
For "image_prompt": Combine components 1-5 and 7 into a single flowing paragraph (skip Audio). STRICT LIMIT: 40-60 words.
"""
                elif gen_video and not gen_image:
                    prompt_framework_block += """
For "video_prompt": Combine ALL 7 components into a single flowing paragraph. Include camera MOVEMENT verbs and Audio descriptions. STRICT LIMIT: 80-120 words.
"""
                else:
                    prompt_framework_block += """
For "image_prompt": Combine components 1-5 and 7 into a single flowing paragraph (skip Audio). Focus on STATIC composition â€” pose, framing, lighting. STRICT LIMIT: 40-60 words.
For "video_prompt": Combine ALL 7 components into a single flowing paragraph. Focus on MOTION â€” camera movement, character action, physics, audio. STRICT LIMIT: 80-120 words.
"""
                prompt_framework_block += "â•â• END FRAMEWORK â•â•\n"

            # Build anchor rules (shared)
            anchor_rules = ""
            if has_anchor:
                anchor_rules = f"""\nâ•â• MANDATORY RULES â•â•
1. Every keyword MUST contain the concept prefix \"{concept_prefix_str}\" OR style modifier \"{style_modifier_str}\"
2. Subjects MUST come from the ALLOWED SUBJECTS list
3. Environments MUST come from the ALLOWED ENVIRONMENTS list
4. Keywords MUST NEVER contain any FORBIDDEN TERMS
5. Keywords must be concrete visual descriptions: what a camera literally sees
6. Use simple English words that Pexels/Pixabay understand
7. NO duplicate keywords across scenes â€” each scene must have UNIQUE footage
8. Keywords should create a coherent visual STORYBOARD: flowing naturally from scene to scene
9. Think: "If I search this on Pexels, will I find matching video clips?"
{('10. image_prompt and video_prompt must follow the 7-COMPONENT FRAMEWORK above â€” flowing narrative paragraphs, NOT tag lists' if (gen_image or gen_video) else '')}"""
            else:
                anchor_rules = f"""\nâ•â• RULES â•â•
1. Keywords MUST describe what a camera literally sees: objects, people, actions, places, lighting
2. Use simple, common English words that stock footage sites understand
3. AVOID abstract concepts, emotions, narrative terms ("journey", "realization", "tension", "discovery")
4. NO duplicate keywords across scenes â€” each scene must have UNIQUE footage
5. Keywords should create a coherent visual STORYBOARD: flowing naturally from scene to scene
6. Think: "If I search this on Pexels, will I find matching video clips?"
{('7. image_prompt and video_prompt must follow the 7-COMPONENT FRAMEWORK above â€” flowing narrative paragraphs, NOT tag lists' if (gen_image or gen_video) else '')}"""

            # Build dynamic task description
            task_parts = []
            if gen_keywords:
                task_parts.append('keywords')
            if gen_image and gen_video:
                task_parts.append('image/video prompts')
            elif gen_image:
                task_parts.append('image prompts')
            elif gen_video:
                task_parts.append('video prompts')
            task_desc = ' + '.join(task_parts) if task_parts else 'keywords'

            # â”€â”€ PER-BATCH LOOP: Build prompt, call AI, parse results â”€â”€
            for batch_idx, batch_scenes in enumerate(scene_batches):
                batch_num = batch_idx + 1
                batch_start_pct = 10 + int(80 * batch_idx / num_batches)
                batch_end_pct = 10 + int(80 * (batch_idx + 1) / num_batches)
                batch_scene_count = len(batch_scenes)

                if num_batches > 1:
                    yield f"data: {json_module.dumps({'type': 'progress', 'message': f'Batch {batch_num}/{num_batches}: {batch_scene_count} scenes', 'current': sum(len(b) for b in scene_batches[:batch_idx]), 'total': total_scenes, 'percentage': batch_start_pct}, ensure_ascii=False)}\n\n"

                # Build per-batch scenes block
                scenes_block_parts = []
                for scene in batch_scenes:
                    meta = scene_meta[scene.scene_id]
                    base = f"Scene {scene.scene_id} ({meta['keyword_count']} keywords needed, ~{meta['target_clip_duration']}s each): {scene.content}"
                    d = dir_map.get(scene.scene_id)
                    if d and d.get('direction_notes'):
                        base += f"\n  [DIRECTION]: {d['direction_notes']}"
                    scenes_block_parts.append(base)
                scenes_block = "\n".join(scenes_block_parts)

                prompt = f"""You are a visual STORYBOARD director. Generate {task_desc} for ALL {batch_scene_count} scenes at once to ensure consistency and flow.
The script is in {lang_name}.
{concept_anchor_block if has_anchor else ''}{consistency_block}{prompt_framework_block}
â•â• SCENES ({batch_scene_count} scenes{f', batch {batch_num}/{num_batches}' if num_batches > 1 else ''}) â•â•
{scenes_block}
{anchor_rules}

Return EXACTLY {batch_scene_count} scene blocks in this format (NO JSON, plain text only):

{example_output}
===SCENE 2===
...

Return ONLY the scene blocks, no other text."""

                print(f"[SceneKeywords] {'Batch ' + str(batch_num) + '/' + str(num_batches) + ': ' if num_batches > 1 else ''}{batch_scene_count} scenes, anchor={has_anchor}, image={gen_image}, video={gen_video}")

                progress_msg = f'AI: {batch_scene_count} scenes'
                parts = []
                if gen_keywords:
                    parts.append('keywords')
                if gen_image and gen_video:
                    parts.append('image/video prompts')
                elif gen_image:
                    parts.append('image prompts')
                elif gen_video:
                    parts.append('video prompts')
                if parts:
                    progress_msg += f' ({", ".join(parts)})'

                yield f"data: {json_module.dumps({'type': 'progress', 'message': progress_msg, 'current': sum(len(b) for b in scene_batches[:batch_idx]), 'total': total_scenes, 'percentage': batch_start_pct + 10}, ensure_ascii=False)}\n\n"

                def call_ai(p=prompt):
                    return ai_client.generate(p, temperature=0.3)

                try:
                    with concurrent.futures.ThreadPoolExecutor() as pool:
                        result_text = await loop.run_in_executor(pool, call_ai)

                    print(f"[SceneKeywords] AI response length: {len(result_text)} chars")
                    scene_blocks = re_mod.split(r'===SCENE\s+(\d+)===', result_text)
                    print(f"[SceneKeywords] re.split produced {len(scene_blocks)} parts (expect {batch_scene_count * 2 + 1} for {batch_scene_count} scenes)")
                    parsed_count = 0
                    if len(scene_blocks) >= 3:
                        for i in range(1, len(scene_blocks), 2):
                            try:
                                scene_id = int(scene_blocks[i])
                            except (ValueError, IndexError):
                                continue
                            block = scene_blocks[i + 1] if i + 1 < len(scene_blocks) else ''
                            entry = {'scene_id': scene_id}
                            meta = scene_meta.get(scene_id, {'keyword_count': 1, 'target_clip_duration': 4.0})

                            kw_multi = re_mod.search(r'KEYWORDS?:\s*(.+)', block)
                            if kw_multi:
                                kw_text = kw_multi.group(1).strip()
                                if '|' in kw_text:
                                    raw_keywords = [k.strip() for k in kw_text.split('|') if k.strip()]
                                else:
                                    raw_keywords = [kw_text]
                                if has_anchor:
                                    raw_keywords = [_validate_keyword_against_concept(kw, request.concept_analysis) for kw in raw_keywords]
                                if len(raw_keywords) > 1:
                                    entry['keywords'] = raw_keywords
                                    entry['target_clip_duration'] = meta['target_clip_duration']
                                entry['keyword'] = raw_keywords[0] if raw_keywords else ''
                                print(f"[SceneKeywords] Scene {scene_id}: keyword='{entry['keyword'][:50]}'")
                            else:
                                entry['keyword'] = ''

                            if gen_image:
                                img_match = re_mod.search(r'IMAGE_PROMPT:\s*(.+?)(?=\n(?:VIDEO_PROMPT:|KEYWORD|===SCENE)|$)', block, re_mod.DOTALL)
                                raw_img = img_match.group(1).strip() if img_match else ''
                                # Enforce 60-word limit for image prompts
                                if raw_img and len(raw_img.split()) > 65:
                                    words = raw_img.split()
                                    raw_img = ' '.join(words[:60])
                                    # Try to end at last sentence boundary
                                    for punc in ['. ', ', ']:
                                        last_punc = raw_img.rfind(punc)
                                        if last_punc > len(raw_img) * 0.6:
                                            raw_img = raw_img[:last_punc + 1]
                                            break
                                entry['image_prompt'] = raw_img
                            if gen_video:
                                vid_match = re_mod.search(r'VIDEO_PROMPT:\s*(.+?)(?=\n(?:IMAGE_PROMPT:|KEYWORD|===SCENE)|$)', block, re_mod.DOTALL)
                                raw_vid = vid_match.group(1).strip() if vid_match else ''
                                # Enforce 120-word limit for video prompts
                                if raw_vid and len(raw_vid.split()) > 130:
                                    words = raw_vid.split()
                                    raw_vid = ' '.join(words[:120])
                                    for punc in ['. ', ', ']:
                                        last_punc = raw_vid.rfind(punc)
                                        if last_punc > len(raw_vid) * 0.6:
                                            raw_vid = raw_vid[:last_punc + 1]
                                            break
                                entry['video_prompt'] = raw_vid
                                print(f"[SceneKeywords] Scene {scene_id}: video_prompt len={len(entry.get('video_prompt', ''))}, words={len(entry.get('video_prompt', '').split())}, first 80: '{entry.get('video_prompt', '')[:80]}'")

                            result_map[scene_id] = entry
                            parsed_count += 1

                    if parsed_count == 0:
                        print(f"[SceneKeywords] Separator format not found, trying JSON fallback...")
                        print(f"[SceneKeywords] AI response first 500 chars: {result_text[:500]}")
                        json_match = re_mod.search(r'\[[\s\S]*\]', result_text)
                        if json_match:
                            try:
                                parsed = json_module.loads(json_match.group())
                                if isinstance(parsed, dict):
                                    parsed = [parsed]
                                for item in parsed:
                                    sid = item.get('scene_id')
                                    if sid is None:
                                        continue
                                    entry = {'scene_id': sid}
                                    entry['keyword'] = item.get('keyword', item.get('keywords', [''])[0] if isinstance(item.get('keywords'), list) else '')
                                    if gen_image and item.get('image_prompt', ''):
                                        raw_img = item['image_prompt'].strip()
                                        if len(raw_img.split()) > 65:
                                            wds = raw_img.split()
                                            raw_img = ' '.join(wds[:60])
                                            for punc in ['. ', ', ']:
                                                lp = raw_img.rfind(punc)
                                                if lp > len(raw_img) * 0.6:
                                                    raw_img = raw_img[:lp + 1]
                                                    break
                                        entry['image_prompt'] = raw_img
                                    if gen_video and item.get('video_prompt', ''):
                                        raw_vid = item['video_prompt'].strip()
                                        if len(raw_vid.split()) > 130:
                                            wds = raw_vid.split()
                                            raw_vid = ' '.join(wds[:120])
                                            for punc in ['. ', ', ']:
                                                lp = raw_vid.rfind(punc)
                                                if lp > len(raw_vid) * 0.6:
                                                    raw_vid = raw_vid[:lp + 1]
                                                    break
                                        entry['video_prompt'] = raw_vid
                                    result_map[sid] = entry
                            except json_module.JSONDecodeError:
                                pass

                    for scene in batch_scenes:
                        if scene.scene_id not in result_map:
                            print(f"[SceneKeywords] Scene {scene.scene_id} missing from AI response, using empty")
                            result_map[scene.scene_id] = _empty_result(scene.scene_id, gen_image, gen_video)

                    # Summary stats
                    vp_count = sum(1 for v in result_map.values() if v.get('video_prompt', '').strip())
                    ip_count = sum(1 for v in result_map.values() if v.get('image_prompt', '').strip())
                    print(f"[SceneKeywords] {'Batch ' + str(batch_num) + ': ' if num_batches > 1 else ''}parsed {parsed_count} scenes (total: {len(result_map)}/{total_scenes}, with video_prompt: {vp_count}, with image_prompt: {ip_count})")

                except Exception as ai_err:
                    print(f"[SceneKeywords] Batch AI error: {ai_err}")
                    import traceback
                    print(traceback.format_exc())
                    for scene in batch_scenes:
                        if scene.scene_id not in result_map:
                            entry = _empty_result(scene.scene_id, gen_image, gen_video)
                            entry['keyword'] = f'(error: {str(ai_err)[:40]})'
                            result_map[scene.scene_id] = entry
            yield f"data: {json_module.dumps({'type': 'progress', 'message': f'Xong: {total_scenes}/{total_scenes} scenes', 'current': total_scenes, 'total': total_scenes, 'percentage': 95}, ensure_ascii=False)}\n\n"

            result_data = {
                "type": "result",
                "success": True,
                "keywords": list(result_map.values()),
                "total": len(result_map),
                "mode": mode_label,
            }
            yield f"event: result\ndata: {json_module.dumps(result_data, ensure_ascii=False)}\n\n"

        except Exception as e:
            import traceback
            print(f"[SceneKeywords] Error: {e}")
            print(traceback.format_exc())
            yield f"event: error\ndata: {json_module.dumps({'success': False, 'error': str(e)}, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


def _empty_result(scene_id: int, gen_image: bool, gen_video: bool) -> dict:
    """Helper to create empty result entry."""
    entry = {'scene_id': scene_id, 'keyword': ''}
    if gen_image:
        entry['image_prompt'] = ''
    if gen_video:
        entry['video_prompt'] = ''
    return entry


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VIDEO PROMPT PIPELINE (5-Step Sequential)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# â”€â”€ Step 1: Analyze Video Direction â”€â”€

class VideoDirectionSceneItem(BaseModel):
    scene_id: int
    content: str

class AnalyzeVideoDirectionRequest(BaseModel):
    scenes: List[VideoDirectionSceneItem]
    language: Optional[str] = 'vi'
    model: Optional[str] = None
    prompt_style: Optional[str] = None
    main_character: Optional[str] = None
    context_description: Optional[str] = None
    sync_analysis: Optional[Dict[str, Any]] = None


@router.post("/analyze-video-direction")
async def analyze_video_direction(request: AnalyzeVideoDirectionRequest):
    """
    Step 1: Analyze text script and create video direction notes per scene.
    Uses the Scene Builder methodology (6 pillars: Subject, Action, Environment, Camera, Lighting, Audio).
    Automatically batches large scene lists (>50 scenes) to avoid token limits and quality degradation.
    """
    if not request.scenes:
        raise HTTPException(status_code=400, detail="KhÃ´ng cÃ³ scene nÃ o")

    BATCH_SIZE = 50  # Max scenes per AI call

    async def event_generator():
        try:
            ai_client = get_configured_ai_client(model=request.model)
            loop = asyncio.get_event_loop()
            import concurrent.futures
            import re as re_mod

            lang_name = {
                'vi': 'Vietnamese', 'en': 'English', 'zh': 'Chinese',
                'ja': 'Japanese', 'ko': 'Korean', 'es': 'Spanish',
                'fr': 'French', 'th': 'Thai', 'pt': 'Portuguese'
            }.get(request.language, 'the same language as the scenes')

            total = len(request.scenes)

            # Split scenes into batches
            batches = [request.scenes[i:i + BATCH_SIZE] for i in range(0, total, BATCH_SIZE)]
            num_batches = len(batches)

            yield f"data: {json_module.dumps({'type': 'progress', 'message': f'Äáº¡o diá»…n: {total} scenes ({num_batches} batch)', 'percentage': 5}, ensure_ascii=False)}\n\n"

            # Build style block once (shared across all batches)
            style_block = ""
            if request.prompt_style:
                style_block += f"\nâ•â• VISUAL STYLE DIRECTION â•â•\n{request.prompt_style}\n"
            if request.main_character:
                style_block += f"\nâ•â• MAIN CHARACTER â•â•\n{request.main_character}\n"
            if request.context_description:
                style_block += f"\nâ•â• ENVIRONMENT/CONTEXT â•â•\n{request.context_description}\n"
            if request.sync_analysis:
                sa = request.sync_analysis
                sync_parts = []
                if sa.get('characters'):
                    chars = "\n".join([f"  - {c.get('name', '')}: {c.get('visual_description', c.get('description', ''))}" for c in sa['characters'][:5]])
                    sync_parts.append(f"SYNC CHARACTERS (maintain consistency):\n{chars}")
                if sa.get('settings'):
                    sets = "\n".join([f"  - {s.get('name', '')}: {s.get('visual_description', s.get('description', ''))}" for s in sa['settings'][:5]])
                    sync_parts.append(f"SYNC SETTINGS (maintain consistency):\n{sets}")
                if sa.get('visual_style'):
                    sync_parts.append(f"SYNC VISUAL STYLE: {sa['visual_style']}")
                if sync_parts:
                    style_block += "\nâ•â• SYNC ANALYSIS (follow these constraints) â•â•\n" + "\n".join(sync_parts) + "\n"

            all_directions = []

            for batch_idx, batch_scenes in enumerate(batches):
                batch_num = batch_idx + 1
                batch_total = len(batch_scenes)
                batch_start_pct = 10 + int((batch_idx / num_batches) * 75)
                batch_end_pct = 10 + int(((batch_idx + 1) / num_batches) * 75)

                yield f"data: {json_module.dumps({'type': 'progress', 'message': f'Batch {batch_num}/{num_batches}: {batch_total} scenes', 'percentage': batch_start_pct}, ensure_ascii=False)}\n\n"

                scenes_block = "\n".join([
                    f"Scene {s.scene_id}: {s.content}" for s in batch_scenes
                ])

                prompt = f"""You are an expert VIDEO DIRECTOR converting a text script into a video script with directing notes.
The script is in {lang_name}. Your output must be in English.

â•â• SCENE BUILDER METHODOLOGY (6 PILLARS) â•â•
For each scene, you must decompose the text into 6 technical elements:
1. SUBJECT: Who is the center? Identity, age, physical features, clothing, distinguishing marks.
2. ACTION: What specific action? One primary action per scene. Avoid abstract verbs.
3. ENVIRONMENT: Where? Time of day? Weather? Spatial details.
4. CAMERA: Shot size (Wide/Medium/Close-up/ECU), movement (Pan/Tilt/Dolly/Tracking), lens (35mm/85mm).
5. LIGHTING: Light source, quality (Volumetric/Rim/High-contrast/Soft), color temperature.
6. AUDIO: Dialogue (in quotes), SFX, ambient sounds, background music mood.

â•â• EMOTION MAPPING â•â•
Translate emotions to visual parameters:
- Positive (Romance/Joy): Golden hour warm light, warm tones, symmetrical composition
- Negative (Sadness/Loneliness): Blue hour cold light, cool tones, distance shots, slow zoom
- Dramatic (Mystery/Tension): High contrast, deep shadows (Film Noir), side lighting

â•â• RULES â•â•
- Each scene should be 4-8 seconds of video
- One primary action OR one camera movement per scene
- Decompose complex actions into multiple shots
{style_block}
â•â• SCENES TO ANALYZE ({batch_total} scenes, batch {batch_num}/{num_batches}) â•â•
{scenes_block}

Return EXACTLY {batch_total} scene blocks in this format (NO JSON, plain text paragraphs):

===SCENE 1===
A cinematic medium shot of a young woman in a red dress walking through a sunlit garden, warm golden hour lighting casting long shadows, camera slowly dollying forward on 35mm lens, birds chirping in background with soft ambient music...

===SCENE 2===
...

Return ONLY the scene blocks with ===SCENE N=== separators, no other text."""

                yield f"data: {json_module.dumps({'type': 'progress', 'message': f'Batch {batch_num}/{num_batches}: AI phÃ¢n tÃ­ch...', 'percentage': batch_start_pct + 5}, ensure_ascii=False)}\n\n"

                with concurrent.futures.ThreadPoolExecutor() as executor:
                    response_text = await loop.run_in_executor(
                        executor,
                        lambda p=prompt: ai_client.generate(p)
                    )

                yield f"data: {json_module.dumps({'type': 'progress', 'message': f'Batch {batch_num}/{num_batches}: xá»­ lÃ½ káº¿t quáº£...', 'percentage': batch_end_pct - 5}, ensure_ascii=False)}\n\n"

                # Parse paragraph separator format: ===SCENE N===\nparagraph...
                scene_blocks = re_mod.split(r'===SCENE\s+(\d+)===', response_text)
                batch_directions = []
                if len(scene_blocks) >= 3:
                    for i in range(1, len(scene_blocks), 2):
                        try:
                            scene_id = int(scene_blocks[i])
                        except (ValueError, IndexError):
                            continue
                        block = scene_blocks[i + 1].strip() if i + 1 < len(scene_blocks) else ''
                        batch_directions.append({"scene_id": scene_id, "direction_notes": block})
                else:
                    # Fallback: try JSON parsing
                    json_match = re_mod.search(r'\[.*\]', response_text, re_mod.DOTALL)
                    if json_match:
                        batch_directions = json_module.loads(json_match.group())
                    else:
                        batch_directions = json_module.loads(response_text)

                all_directions.extend(batch_directions)
                print(f"[VideoDirection] Batch {batch_num}/{num_batches}: parsed {len(batch_directions)} directions (running total: {len(all_directions)}/{total})")

            yield f"data: {json_module.dumps({'type': 'progress', 'message': f'Xong: {len(all_directions)}/{total} scenes', 'percentage': 90}, ensure_ascii=False)}\n\n"

            result_data = {
                "type": "result",
                "success": True,
                "directions": all_directions,
                "total": len(all_directions),
            }
            yield f"event: result\ndata: {json_module.dumps(result_data, ensure_ascii=False)}\n\n"

        except Exception as e:
            import traceback
            print(f"[VideoDirection] Error: {e}")
            print(traceback.format_exc())
            yield f"event: error\ndata: {json_module.dumps({'success': False, 'error': str(e)}, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive", "X-Accel-Buffering": "no"}
    )


# â”€â”€ Step 2: Extract Entities (Characters, Settings, Props appearing â‰¥2 times) â”€â”€

class ExtractEntitiesRequest(BaseModel):
    video_prompts: List[Dict[str, Any]]  # [{scene_id, video_prompt, direction_notes?}]
    language: Optional[str] = 'vi'
    model: Optional[str] = None
    script_scenes: Optional[List[Dict[str, Any]]] = None  # Original script scenes for name extraction


@router.post("/extract-entities")
async def extract_entities(request: ExtractEntitiesRequest):
    """
    Step 3: Scan all video prompts and extract characters, settings, props
    that appear 2+ times. Assign a single-word name (min 3 chars) from the script.
    """
    if not request.video_prompts:
        raise HTTPException(status_code=400, detail="KhÃ´ng cÃ³ video prompts")

    async def event_generator():
        try:
            ai_client = get_configured_ai_client(model=request.model)
            loop = asyncio.get_event_loop()
            import concurrent.futures

            total = len(request.video_prompts)
            yield f"data: {json_module.dumps({'type': 'progress', 'message': f'TrÃ­ch xuáº¥t entities: {total} prompts', 'percentage': 5}, ensure_ascii=False)}\n\n"

            prompts_block = "\n".join([
                f"Scene {p.get('scene_id', i+1)}: {p.get('video_prompt', p.get('direction_notes', ''))}"
                for i, p in enumerate(request.video_prompts)
            ])

            # Include original script text for name extraction
            script_block = ""
            if request.script_scenes:
                script_block = "\nâ•â• ORIGINAL SCRIPT (for extracting names) â•â•\n" + "\n".join([
                    f"Scene {s.get('scene_id', i+1)}: {s.get('content', '')}"
                    for i, s in enumerate(request.script_scenes)
                ]) + "\n"

            prompt = f"""You are an entity extraction specialist for video production.

Analyze ALL the video prompts below and identify recurring entities that appear in 2 or MORE scenes.

ENTITY TYPES:
1. **character** â€” People, animals, recurring figures
2. **environment** â€” Locations, settings, rooms, landscapes
3. **prop** â€” Objects, tools, vehicles, items

NAMING RULES:
- Extract the name from the original script text below (use a single word, minimum 3 characters)
- If no name exists in the script, create a descriptive English name (min 3 chars)
- Names must be unique, short, and recognizable
- Format: CamelCase, e.g. "Minh", "Office", "RedCar", "Beach"
{script_block}
â•â• VIDEO PROMPTS ({total} scenes) â•â•
{prompts_block}

â•â• OUTPUT FORMAT â•â•
Return a JSON object:
{{
  "entities": [
    {{
      "name": "EntityName",
      "type": "character|environment|prop",
      "description": "Detailed visual description used consistently across scenes",
      "scene_ids": [1, 3, 5],
      "count": 3
    }}
  ]
}}

RULES:
- ONLY include entities appearing in 2+ scenes
- Each entity must have a detailed visual description
- For characters: include age, gender, hair, skin, distinguishing features
- For environments: include spatial details, lighting, atmosphere
- For props: include color, size, material, distinctive features
- Sort by count (most frequent first)

Return ONLY the JSON object, no other text."""

            yield f"data: {json_module.dumps({'type': 'progress', 'message': 'AI phÃ¢n tÃ­ch entities...', 'percentage': 30}, ensure_ascii=False)}\n\n"

            with concurrent.futures.ThreadPoolExecutor() as executor:
                response_text = await loop.run_in_executor(
                    executor,
                    lambda: ai_client.generate(prompt)
                )

            yield f"data: {json_module.dumps({'type': 'progress', 'message': 'Xá»­ lÃ½ káº¿t quáº£...', 'percentage': 80}, ensure_ascii=False)}\n\n"

            import re as re_mod
            json_match = re_mod.search(r'\{.*\}', response_text, re_mod.DOTALL)
            if json_match:
                entities_data = json_module.loads(json_match.group())
            else:
                entities_data = json_module.loads(response_text)

            entities = entities_data.get('entities', [])

            # Validate min 3 chars for names
            for entity in entities:
                if len(entity.get('name', '')) < 3:
                    entity['name'] = entity.get('name', 'Ent') + 'Ref'

            result_data = {
                "type": "result",
                "success": True,
                "entities": entities,
                "total": len(entities),
            }
            yield f"event: result\ndata: {json_module.dumps(result_data, ensure_ascii=False)}\n\n"

        except Exception as e:
            import traceback
            print(f"[ExtractEntities] Error: {e}")
            print(traceback.format_exc())
            yield f"event: error\ndata: {json_module.dumps({'success': False, 'error': str(e)}, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive", "X-Accel-Buffering": "no"}
    )


# â”€â”€ Step 4: Generate Reference Image Prompts (VEO3 methodology) â”€â”€

class GenerateReferencePromptsRequest(BaseModel):
    entities: List[Dict[str, Any]]  # [{name, type, description, ...}]
    model: Optional[str] = None
    prompt_style: Optional[str] = None
    sync_analysis: Optional[Dict[str, Any]] = None  # From metadata style analysis


@router.post("/generate-reference-prompts")
async def generate_reference_prompts(request: GenerateReferencePromptsRequest):
    """
    Step 4: Generate multi-panel reference sheet prompts for each entity.
    - Characters: 1 prompt â†’ 3-panel sheet (front + three-quarter + profile) on white background
    - Environments: 1 prompt â†’ 2-panel sheet (establishing + detail close-up)
    - Props: 1 prompt â†’ 2-panel sheet (full product + macro detail)
    Output has 2 parts: entity names list + reference prompts.
    """
    if not request.entities:
        raise HTTPException(status_code=400, detail="KhÃ´ng cÃ³ entities")

    async def event_generator():
        try:
            ai_client = get_configured_ai_client(model=request.model)
            loop = asyncio.get_event_loop()
            import concurrent.futures

            total = len(request.entities)
            yield f"data: {json_module.dumps({'type': 'progress', 'message': f'Táº¡o reference: {total} entities', 'percentage': 5}, ensure_ascii=False)}\n\n"

            # Build entity descriptions
            entities_block = ""
            for e in request.entities:
                entities_block += f"- [{e['name']}] (type: {e['type']}): {e.get('description', '')}\n"

            # Sync analysis context
            sync_block = ""
            if request.sync_analysis:
                sa = request.sync_analysis
                if sa.get('characters'):
                    chars = ", ".join([c.get('name', '') for c in sa['characters'][:5]])
                    sync_block += f"\nSYNC ANALYSIS CHARACTERS: {chars}\n"
                if sa.get('settings'):
                    sets = ", ".join([s.get('name', '') for s in sa['settings'][:5]])
                    sync_block += f"SYNC ANALYSIS SETTINGS: {sets}\n"
                if sa.get('visual_style'):
                    sync_block += f"SYNC ANALYSIS VISUAL STYLE: {sa['visual_style']}\n"

            style_block = ""
            if request.prompt_style:
                style_block = f"\nâ•â• VISUAL STYLE TO APPLY â•â•\n{request.prompt_style}\n"

            prompt = f"""You are a VEO 3.1 Reference Image Specialist.
Generate ONE single multi-panel reference sheet prompt per entity. Each prompt describes a SINGLE IMAGE containing multiple panels/views arranged side by side.
{sync_block}{style_block}
â•â• ENTITIES â•â•
{entities_block}

â•â• MULTI-PANEL REFERENCE SHEET RULES â•â•

FOR CHARACTERS (type=character):
Generate 1 prompt describing a 3-panel reference sheet in ONE image:
"A professional character reference sheet on a pure white background, divided into three equal panels arranged horizontally. LEFT PANEL: front-facing portrait of [FULL DESCRIPTION], facing directly toward the camera, neutral expression, eyes looking straight into the lens, [CLOTHING], full visible face with no obstructions. CENTER PANEL: three-quarter view of the same [CHARACTER], turned approximately 45 degrees to the right, showing depth and volume of the face, the transition between nose bridge and cheekbone clearly visible, ear partially visible, same [CLOTHING]. RIGHT PANEL: side profile of the same [CHARACTER], facing 90 degrees to the right, showing full silhouette, clear definition of chin projection, nose shape, and hair thickness. All three panels share identical soft diffused studio lighting, no harsh shadows, 4K resolution, sharp details, professional quality, consistent appearance across all panels."

FOR ENVIRONMENTS (type=environment):
Generate 1 prompt describing a 2-panel reference sheet:
"A cinematic environment reference sheet divided into two panels. LEFT PANEL: wide establishing shot of [DETAILED ENVIRONMENT], [TIME OF DAY] lighting with [LIGHT DESCRIPTION], shot on 35mm wide-angle lens, [COLOR GRADE], 16:9 widescreen, no people. RIGHT PANEL: detail close-up crop of a key architectural or textural element from the same environment, showing surface materials and lighting interaction. Both panels share consistent color grading and atmosphere, 4K quality."

FOR PROPS (type=prop):
Generate 1 prompt describing a 2-panel reference sheet:
"A professional product reference sheet on a pure white background, divided into two panels. LEFT PANEL: full product shot of [DETAILED PROP DESCRIPTION], soft studio lighting from above and both sides, no harsh shadows, clean composition. RIGHT PANEL: macro close-up detail shot of the same [PROP] showing surface texture, material quality, and fine details. Shot on 100mm macro lens, sharp details, 4K quality, consistent lighting across both panels."

â•â• WORD COUNT LIMIT â•â•
Each reference sheet prompt must be 60-100 words. Be precise â€” every word must add visual information.

â•â• OUTPUT FORMAT â•â•
Return a JSON object:
{{
  "entities_header": "[Name1], [Name2], [Name3], ...",
  "reference_prompts": [
    {{
      "entity_name": "EntityName",
      "entity_type": "character|environment|prop",
      "prompts": [
        {{"angle": "REFERENCE_SHEET", "prompt": "The full multi-panel reference sheet prompt..."}}
      ]
    }}
  ]
}}

Each entity has EXACTLY 1 prompt with angle "REFERENCE_SHEET". The prompt describes ALL panels within ONE single image.

Return ONLY the JSON object, no other text."""

            yield f"data: {json_module.dumps({'type': 'progress', 'message': 'AI táº¡o reference prompts...', 'percentage': 30}, ensure_ascii=False)}\n\n"

            with concurrent.futures.ThreadPoolExecutor() as executor:
                response_text = await loop.run_in_executor(
                    executor,
                    lambda: ai_client.generate(prompt)
                )

            yield f"data: {json_module.dumps({'type': 'progress', 'message': 'Xá»­ lÃ½ káº¿t quáº£...', 'percentage': 85}, ensure_ascii=False)}\n\n"

            import re as re_mod
            json_match = re_mod.search(r'\{.*\}', response_text, re_mod.DOTALL)
            if json_match:
                ref_data = json_module.loads(json_match.group())
            else:
                ref_data = json_module.loads(response_text)

            result_data = {
                "type": "result",
                "success": True,
                "entities_header": ref_data.get('entities_header', ''),
                "reference_prompts": ref_data.get('reference_prompts', []),
                "total": len(ref_data.get('reference_prompts', [])),
            }
            yield f"event: result\ndata: {json_module.dumps(result_data, ensure_ascii=False)}\n\n"

        except Exception as e:
            import traceback
            print(f"[ReferencePrompts] Error: {e}")
            print(traceback.format_exc())
            yield f"event: error\ndata: {json_module.dumps({'success': False, 'error': str(e)}, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive", "X-Accel-Buffering": "no"}
    )


# â”€â”€ Step 5: Generate Scene Builder Prompts (inject [Name] references) â”€â”€

class GenerateSceneBuilderPromptsRequest(BaseModel):
    video_prompts: List[Dict[str, Any]]  # [{scene_id, video_prompt}]
    entities: List[Dict[str, Any]]  # [{name, type, description, scene_ids}]
    directions: Optional[List[Dict[str, Any]]] = None  # From step 1
    model: Optional[str] = None
    prompt_style: Optional[str] = None
    sync_analysis: Optional[Dict[str, Any]] = None


@router.post("/generate-scene-builder-prompts")
async def generate_scene_builder_prompts(request: GenerateSceneBuilderPromptsRequest):
    """
    Step 5: Take video prompts + entity list, inject [Name] references
    to create Scene Builder prompts that link to reference images.
    """
    if not request.video_prompts:
        raise HTTPException(status_code=400, detail="KhÃ´ng cÃ³ video prompts")

    async def event_generator():

        try:
            ai_client = get_configured_ai_client(model=request.model)
            loop = asyncio.get_event_loop()
            import concurrent.futures
            import re as re_mod

            total = len(request.video_prompts)
            yield f"data: {json_module.dumps({'type': 'progress', 'message': f'Scene builder: {total} scenes', 'percentage': 5}, ensure_ascii=False)}\n\n"

            # Build entity reference map
            entity_names = ", ".join([f"[{e['name']}]" for e in request.entities])
            entity_descriptions = "\n".join([
                f"- [{e['name']}] ({e['type']}): {e.get('description', '')}"
                for e in request.entities
            ])

            # Build entity-to-scene mapping
            entity_scene_map = {}
            for e in request.entities:
                for sid in e.get('scene_ids', []):
                    if sid not in entity_scene_map:
                        entity_scene_map[sid] = []
                    entity_scene_map[sid].append(e['name'])

            # Include direction notes if available
            direction_block = ""
            if request.directions:
                direction_block = "\nâ•â• DIRECTION NOTES â•â•\n" + "\n".join([
                    f"Scene {d.get('scene_id', 0)}: {d.get('direction_notes', '')}"
                    for d in request.directions
                ]) + "\n"

            style_block = ""
            if request.prompt_style:
                style_block = f"\nâ•â• VISUAL STYLE â•â•\n{request.prompt_style}\n"

            sync_block = ""
            if request.sync_analysis:
                sa = request.sync_analysis
                sync_sb_parts = []
                if sa.get('characters'):
                    chars = "\n".join([f"  - {c.get('name', '')}: {c.get('visual_description', c.get('description', ''))}" for c in sa['characters'][:5]])
                    sync_sb_parts.append(f"SYNC CHARACTERS (maintain consistency):\n{chars}")
                if sa.get('settings'):
                    sets = "\n".join([f"  - {s.get('name', '')}: {s.get('visual_description', s.get('description', ''))}" for s in sa['settings'][:5]])
                    sync_sb_parts.append(f"SYNC SETTINGS (maintain consistency):\n{sets}")
                if sa.get('visual_style'):
                    sync_sb_parts.append(f"SYNC VISUAL STYLE: {sa['visual_style']}")
                if sync_sb_parts:
                    sync_block = "\nâ•â• SYNC ANALYSIS (follow for consistency) â•â•\n" + "\n".join(sync_sb_parts) + "\n"

            # Batching: split into batches of 50 scenes
            BATCH_SIZE = 50
            scene_batches = [request.video_prompts[i:i + BATCH_SIZE] for i in range(0, total, BATCH_SIZE)]
            num_batches = len(scene_batches)
            all_results = []

            for batch_idx, batch_prompts in enumerate(scene_batches):
                batch_num = batch_idx + 1
                batch_count = len(batch_prompts)
                batch_start_pct = 10 + int(70 * batch_idx / num_batches)

                if num_batches > 1:
                    yield f"data: {json_module.dumps({'type': 'progress', 'message': f'Batch {batch_num}/{num_batches}: {batch_count} scenes', 'current': sum(len(b) for b in scene_batches[:batch_idx]), 'total': total, 'percentage': batch_start_pct}, ensure_ascii=False)}\n\n"

                # Build per-batch prompts block
                prompts_block = ""
                for p in batch_prompts:
                    sid = p.get('scene_id', 0)
                    entities_in_scene = entity_scene_map.get(sid, [])
                    entities_hint = f" (entities: {', '.join(['['+n+']' for n in entities_in_scene])})" if entities_in_scene else ""
                    prompts_block += f"Scene {sid}{entities_hint}: {p.get('video_prompt', '')}\n"

                prompt = f"""You are a Scene Builder IMAGE specialist for Google VEO 3.

Your task: Generate a SCENE IMAGE PROMPT for each scene that uses [Name] reference tokens for entities.
These are IMAGE prompts â€” describing a STATIC scene composition (not video). They serve as visual blueprints for VEO3 scene generation.

â•â• AVAILABLE ENTITY REFERENCES â•â•
{entity_names}

Entity details:
{entity_descriptions}
{direction_block}{style_block}{sync_block}
â•â• VIDEO PROMPTS (context for {batch_count} scenes{f', batch {batch_num}/{num_batches}' if num_batches > 1 else ''}) â•â•
{prompts_block}

â•â• SCENE BUILDER IMAGE PROMPT RULES â•â•
1. Generate an IMAGE prompt for each scene (STATIC composition, not video)
2. Use [Name] tokens to reference entities instead of full descriptions:
   - Character: "[MinhAnh] stands in front of..."
   - Environment: "in [Office], afternoon golden hour lighting..."
   - Prop: "[Journal] lies open on the table..."

3. Focus on STATIC image composition:
   - Pose, position, framing (not movement or camera motion)
   - Lighting, color palette, mood
   - Spatial arrangement of characters and props within the environment
   - Shot size: Wide/Medium/Close-up/ECU
   - Lens: 35mm/50mm/85mm

4. Each image prompt should capture the KEY VISUAL MOMENT of the scene
5. Write as ONE FLOWING PARAGRAPH â€” not bullet points or tags
6. All prompts MUST be in English
7. **WORD COUNT LIMIT: 40-60 words per scene builder prompt.** Be precise and dense â€” every word must add visual information.

â•â• OUTPUT FORMAT â•â•
Return EXACTLY {batch_count} scene blocks in this format (NO JSON, plain text only):

===SCENE 1===
SCENE_BUILDER_PROMPT: A cinematic medium close-up, shot on 35mm lens. [MinhAnh] sits at a desk in [Office], looking up from [Laptop] and smiling gently, soft golden hour light streaming through the window, warm color palette, shallow depth of field...

===SCENE 2===
SCENE_BUILDER_PROMPT: ...

Return ONLY the scene blocks, no other text."""

                batch_label = f' (batch {batch_num}/{num_batches})' if num_batches > 1 else ''
                yield f"data: {json_module.dumps({'type': 'progress', 'message': f'AI táº¡o scene builder{batch_label}...', 'percentage': batch_start_pct + 10}, ensure_ascii=False)}\n\n"

                with concurrent.futures.ThreadPoolExecutor() as executor:
                    response_text = await loop.run_in_executor(
                        executor,
                        lambda p=prompt: ai_client.generate(p)
                    )

                # Parse paragraph separator format
                scene_blocks = re_mod.split(r'===SCENE\s+(\d+)===', response_text)
                parsed_count = 0

                if len(scene_blocks) >= 3:
                    for i in range(1, len(scene_blocks), 2):
                        try:
                            scene_id = int(scene_blocks[i])
                        except (ValueError, IndexError):
                            continue
                        block = scene_blocks[i + 1] if i + 1 < len(scene_blocks) else ''
                        sb_match = re_mod.search(r'SCENE_BUILDER_PROMPT:\s*(.+?)(?=\n===SCENE|$)', block, re_mod.DOTALL)
                        sb_prompt = sb_match.group(1).strip() if sb_match else block.strip()
                        all_results.append({
                            'scene_id': scene_id,
                            'scene_builder_prompt': sb_prompt
                        })
                        parsed_count += 1

                # Fallback: try JSON parsing if separator format failed
                if parsed_count == 0:
                    print(f"[SceneBuilder] Separator format not found, trying JSON fallback...")
                    json_match = re_mod.search(r'\[.*\]', response_text, re_mod.DOTALL)
                    if json_match:
                        try:
                            parsed = json_module.loads(json_match.group())
                            if isinstance(parsed, list):
                                all_results.extend(parsed)
                                parsed_count = len(parsed)
                        except json_module.JSONDecodeError:
                            pass

                print(f"[SceneBuilder] {'Batch ' + str(batch_num) + ': ' if num_batches > 1 else ''}parsed {parsed_count} scenes (total: {len(all_results)}/{total})")

            yield f"data: {json_module.dumps({'type': 'progress', 'message': 'Xá»­ lÃ½ káº¿t quáº£...', 'percentage': 90}, ensure_ascii=False)}\n\n"

            result_data = {
                "type": "result",
                "success": True,
                "scene_builder_prompts": all_results,
                "total": len(all_results),
            }
            yield f"event: result\ndata: {json_module.dumps(result_data, ensure_ascii=False)}\n\n"

        except Exception as e:
            import traceback
            print(f"[SceneBuilder] Error: {e}")
            print(traceback.format_exc())
            yield f"event: error\ndata: {json_module.dumps({'success': False, 'error': str(e)}, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive", "X-Accel-Buffering": "no"}
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SSE STREAMING ENDPOINTS - Real-time progress
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.post("/analyze-to-style-a-stream")
async def analyze_to_style_a_stream(request: AnalyzeToStyleARequest):
    """
    SSE streaming endpoint for StyleA analysis.
    Sends real-time progress events to the frontend.
    """
    import logging
    logger = logging.getLogger(__name__)
    
    if not request.scripts or len(request.scripts) < 1:
        raise HTTPException(status_code=400, detail="Cáº§n Ã­t nháº¥t 1 ká»‹ch báº£n máº«u")
    
    if len(request.scripts) > 20:
        raise HTTPException(status_code=400, detail="Tá»‘i Ä‘a 20 ká»‹ch báº£n máº«u")
    
    for i, script in enumerate(request.scripts):
        if len(script.strip()) < 50:
            raise HTTPException(status_code=400, detail=f"Ká»‹ch báº£n #{i+1} cáº§n Ã­t nháº¥t 50 kÃ½ tá»±")
    
    progress_queue: asyncio.Queue = asyncio.Queue()
    main_loop = asyncio.get_running_loop()
    
    def progress_callback(step: str, percentage: int, message: str):
        """Thread-safe callback that puts progress into the async queue"""
        main_loop.call_soon_threadsafe(
            progress_queue.put_nowait,
            {"step": step, "percentage": percentage, "message": message}
        )
    
    async def event_generator():
        try:
            ai_client = get_configured_ai_client(model=request.model)
            analyzer = ConversationStyleAnalyzer(ai_client)
            
            loop = asyncio.get_event_loop()
            
            # â”€â”€ PRE-TRANSLATION: Translate scripts to output language if needed â”€â”€
            scripts_to_analyze = list(request.scripts)
            output_lang = request.output_language.strip()
            
            if output_lang:
                # Auto-detect input language from first script
                detected_input = _detect_language(request.scripts[0])
                
                if detected_input != output_lang:
                    lang_names = {"en": "English", "vi": "Vietnamese", "ja": "Japanese", "ko": "Korean", "zh": "Chinese", "es": "Spanish", "fr": "French", "th": "Thai", "de": "German", "pt": "Portuguese", "ru": "Russian"}
                    src_name = lang_names.get(detected_input, detected_input)
                    out_name = lang_names.get(output_lang, output_lang)
                    
                    progress_callback("translate", 2, f"Äang dá»‹ch {len(scripts_to_analyze)} script {src_name} â†’ {out_name}...")
                    
                    translated_scripts = []
                    for i, script in enumerate(scripts_to_analyze):
                        try:
                            translate_prompt = f"""You are a professional translator. Translate the following script from {src_name} to {out_name}.

Rules:
- Preserve the STRUCTURE (paragraphs, line breaks) exactly.
- Prioritize NATURAL, fluent {out_name} over rigid literal translation.
- Adapt idioms and cultural references to feel natural in {out_name}.
- Keep proper nouns and technical terms unchanged.
- Output ONLY the translated text, no commentary.

--- SCRIPT ---
{script}
--- END ---

Translated {out_name} script:"""
                            translated = await loop.run_in_executor(
                                None,
                                lambda p=translate_prompt: ai_client.generate(p, temperature=0.3)
                            )
                            if translated and len(translated.strip()) > 50:
                                translated_scripts.append(translated.strip())
                            else:
                                translated_scripts.append(script)  # fallback
                        except Exception as e:
                            import logging
                            logging.getLogger(__name__).warning(f"Translation failed for script {i+1}: {e}")
                            translated_scripts.append(script)  # fallback
                        
                        progress_callback("translate", 2 + int(8 * (i + 1) / len(scripts_to_analyze)), f"ÄÃ£ dá»‹ch {i+1}/{len(scripts_to_analyze)} script")
                    
                    scripts_to_analyze = translated_scripts
                    progress_callback("translate_done", 10, f"ÄÃ£ dá»‹ch xong {len(translated_scripts)} script sang {out_name}")
            
            # Run analysis in thread with progress callback
            task = loop.run_in_executor(
                None,
                lambda: analyzer.analyze_to_style_a(
                    scripts_to_analyze, 
                    progress_callback=progress_callback,
                    analysis_language=request.analysis_language
                )
            )
            
            # Yield progress events while waiting for completion
            while not task.done():
                try:
                    progress = await asyncio.wait_for(progress_queue.get(), timeout=1.0)
                    yield f"data: {json_module.dumps(progress, ensure_ascii=False)}\n\n"
                except asyncio.TimeoutError:
                    # Send heartbeat to keep connection alive
                    yield f": heartbeat\n\n"
            
            # Drain remaining progress events
            while not progress_queue.empty():
                progress = progress_queue.get_nowait()
                yield f"data: {json_module.dumps(progress, ensure_ascii=False)}\n\n"
            
            # Get the result
            style_a, individual_analyses = task.result()
            
            # Send final result as a special event
            result_data = {
                "success": True,
                "style_a": style_a.to_dict(),
                "individual_analyses": individual_analyses,
                "scripts_analyzed": len(individual_analyses)
            }
            yield f"event: result\ndata: {json_module.dumps(result_data, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            import traceback
            error_data = {
                "success": False,
                "error": str(e),
                "traceback": traceback.format_exc()
            }
            yield f"event: error\ndata: {json_module.dumps(error_data, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


@router.post("/advanced-remake/full-pipeline-conversation-stream")
async def advanced_full_pipeline_conversation_stream(request: AdvancedFullPipelineConversationRequest):
    """
    SSE streaming endpoint for conversation-based pipeline.
    Sends real-time progress events as each step completes.
    """
    if not request.original_script or len(request.original_script.strip()) < 50:
        raise HTTPException(status_code=400, detail="Ká»‹ch báº£n gá»‘c cáº§n Ã­t nháº¥t 50 kÃ½ tá»±")
    
    progress_queue: asyncio.Queue = asyncio.Queue()
    main_loop = asyncio.get_running_loop()
    
    def progress_callback(step: str, percentage: int, message: str):
        """Thread-safe callback that puts progress into the async queue"""
        main_loop.call_soon_threadsafe(
            progress_queue.put_nowait,
            {"step": step, "percentage": percentage, "message": message}
        )
    
    async def event_generator():
        try:
            logger.info(f"[STREAM] Starting pipeline. Model={request.model}, script_len={len(request.original_script)}")
            ai_client = get_configured_ai_client(model=request.model)
            workflow = AdvancedRemakeWorkflow(ai_client)
            
            loop = asyncio.get_event_loop()
            
            # Run pipeline in thread with progress callback
            task = loop.run_in_executor(
                None,
                lambda: workflow.full_pipeline_conversation(
                    original_script=request.original_script,
                    target_word_count=request.target_word_count,
                    source_language=request.source_language,
                    language=request.language,
                    dialect=request.dialect,
                    channel_name=request.channel_name,
                    country=request.country,
                    add_quiz=request.add_quiz,
                    value_type=request.value_type,
                    storytelling_style=request.storytelling_style,
                    narrative_voice=request.narrative_voice,
                    custom_narrative_voice=request.custom_narrative_voice,
                    audience_address=request.audience_address,
                    custom_audience_address=request.custom_audience_address,
                    style_profile=request.style_profile,
                    custom_value=request.custom_value,
                    progress_callback=progress_callback
                )
            )
            
            # Yield progress events while waiting for completion
            while not task.done():
                try:
                    progress = await asyncio.wait_for(progress_queue.get(), timeout=2.0)
                    yield f"data: {json_module.dumps(progress, ensure_ascii=False)}\n\n"
                except asyncio.TimeoutError:
                    yield f": heartbeat\n\n"
            
            # Drain remaining progress events
            while not progress_queue.empty():
                progress = progress_queue.get_nowait()
                yield f"data: {json_module.dumps(progress, ensure_ascii=False)}\n\n"
            
            # Get the result (may raise if pipeline failed)
            results = task.result()
            
            # Build response same as original endpoint
            response_data = {
                "success": True,
                "final_script": results.get("final_script", ""),
                "word_count": results.get("word_count", 0)
            }
            
            if "original_analysis" in results:
                response_data["original_analysis"] = results["original_analysis"].to_dict() if hasattr(results["original_analysis"], "to_dict") else results["original_analysis"]
            if "structure_analysis" in results:
                response_data["structure_analysis"] = results["structure_analysis"].to_dict() if hasattr(results["structure_analysis"], "to_dict") else results["structure_analysis"]
            if "outline_a" in results:
                response_data["outline_a"] = results["outline_a"].to_dict() if hasattr(results["outline_a"], "to_dict") else results["outline_a"]
            if "draft_sections" in results:
                response_data["draft_sections"] = [d.to_dict() if hasattr(d, "to_dict") else d for d in results["draft_sections"]]
            if "refined_sections" in results:
                response_data["refined_sections"] = [r.to_dict() if hasattr(r, "to_dict") else r for r in results["refined_sections"]]
            
            logger.info(f"[STREAM] Pipeline complete! {response_data.get('word_count', 0)} words")
            yield f"event: result\ndata: {json_module.dumps(response_data, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            import traceback
            error_msg = str(e)
            tb = traceback.format_exc()
            logger.error(f"[STREAM] Pipeline FAILED: {error_msg}")
            logger.error(f"[STREAM] Traceback:\n{tb}")
            error_data = {
                "success": False,
                "error": error_msg,
                "traceback": tb
            }
            yield f"event: error\ndata: {json_module.dumps(error_data, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# YOUTUBE METADATA GENERATION - Title, Description, Thumbnail Prompt
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class GenerateYoutubeMetadataRequest(BaseModel):
    script: str
    style_profile: Optional[Dict[str, Any]] = None
    title_samples: List[str] = []
    description_samples: List[str] = []
    generate_title: bool = True
    generate_description: bool = True
    generate_thumbnail_prompt: bool = True
    model: Optional[str] = None
    custom_cta: Optional[str] = None  # User's custom CTA template for descriptions
    sync_analysis: Optional[Dict[str, Any]] = None  # Sync analysis data for thumbnail


@router.post("/generate-youtube-metadata")
async def generate_youtube_metadata(request: GenerateYoutubeMetadataRequest):
    """
    Generate YouTube metadata (Title, Description, Thumbnail Prompt)
    based on the generated script, style profile, and SEO best practices.
    
    Uses AI to create optimized:
    - Title: Hook-based, curiosity-driven, SEO-optimized (~60 chars)
    - Description: Multi-layered structure with timestamps, CTAs, keywords
    - Thumbnail Prompt: Visual storytelling prompt with 7-component formula
    """
    if not request.script or len(request.script.strip()) < 50:
        raise HTTPException(status_code=400, detail="Script cáº§n Ã­t nháº¥t 50 kÃ½ tá»±")

    try:
        ai_client = get_configured_ai_client(model=request.model)
        
        # Build context from style profile
        style_context = ""
        if request.style_profile:
            style_summary = request.style_profile.get("style_summary", "")
            tone = request.style_profile.get("tone", "")
            if style_summary:
                style_context += f"\n\nPhong cÃ¡ch kÃªnh: {style_summary}"
            if tone:
                style_context += f"\nTone: {tone}"

        # Script preview (first 2000 chars to save tokens)
        script_preview = request.script.strip()[:2000]
        
        results = {
            "success": True,
            "title": "",
            "description": "",
            "thumbnail_prompt": "",
        }
        
        # â”€â”€ Generate Title â”€â”€
        if request.generate_title:
            title_samples_text = ""
            if request.title_samples:
                title_samples_text = "\n\nMáº«u title tham kháº£o tá»« kÃªnh:\n" + "\n".join(
                    f"- {s}" for s in request.title_samples[:10]
                )
            
            title_prompt = f"""Báº¡n lÃ  chuyÃªn gia YouTube SEO. HÃ£y táº¡o 1 title tá»‘i Æ°u cho video YouTube dá»±a trÃªn ká»‹ch báº£n sau.

YÃŠU Cáº¦U TITLE:
- Tá»‘i Ä‘a 60 kÃ½ tá»± (quan trá»ng cho SEO)
- Sá»­ dá»¥ng hook formula: [Power Word] + [Chá»§ Ä‘á»] + [Curiosity Gap/Number]
- Táº¡o sá»± tÃ² mÃ², khiáº¿n ngÆ°á»i xem PHáº¢I click
- KhÃ´ng clickbait quÃ¡ má»©c, pháº£i pháº£n Ã¡nh Ä‘Ãºng ná»™i dung
- DÃ¹ng power words: BÃ­ máº­t, Sá»± tháº­t, KhÃ´ng ngá», CÃ¡ch, Táº¡i sao, Top...
- Viáº¿t báº±ng tiáº¿ng Viá»‡t (trá»« khi ká»‹ch báº£n báº±ng ngÃ´n ngá»¯ khÃ¡c)
{style_context}
{title_samples_text}

Ká»ŠÌ£CH Báº¢N:
{script_preview}

CHá»ˆ TRáº¢ Vá»€ ÄÃšNG 1 TITLE, KHÃ”NG giáº£i thÃ­ch, KHÃ”NG Ä‘Ã¡nh sá»‘, KHÃ”NG thÃªm dáº¥u ngoáº·c kÃ©p."""

            try:
                title_result = await asyncio.to_thread(
                    ai_client.generate, title_prompt, temperature=0.8
                )
                results["title"] = title_result.strip().strip('"').strip("'").strip()
                logger.info(f"[YT-Meta] Title generated: {results['title']}")
            except Exception as e:
                logger.error(f"[YT-Meta] Title generation failed: {e}")
                results["title"] = f"[Lá»—i táº¡o title: {str(e)[:100]}]"
        
        # â”€â”€ Generate Description â”€â”€
        if request.generate_description:
            desc_samples_text = ""
            if request.description_samples:
                desc_samples_text = "\n\nMáº«u description tham kháº£o tá»« kÃªnh:\n" + "\n".join(
                    f"---\n{s}" for s in request.description_samples[:5]
                )
            
            desc_prompt = f"""Báº¡n lÃ  chuyÃªn gia YouTube SEO. HÃ£y táº¡o description tá»‘i Æ°u cho video YouTube dá»±a trÃªn ká»‹ch báº£n sau.

âš ï¸ QUY Táº®C QUAN TRá»ŒNG Vá»€ CTA/CÃ NHÃ‚N HÃ“A:
- PhÃ¢n tÃ­ch cÃ¡c máº«u description bÃªn dÆ°á»›i (náº¿u cÃ³)
- PHÃT HIá»†N vÃ  LOáº I Bá» táº¥t cáº£ ná»™i dung CÃ NHÃ‚N/Tá»” CHá»¨C tá»« máº«u gá»‘c:
  + TÃªn kÃªnh cá»¥ thá»ƒ, link YouTube/social media cá»¥ thá»ƒ
  + Handle (@username), email liÃªn há»‡
  + Tagline/slogan Ä‘áº·c trÆ°ng cá»§a kÃªnh gá»‘c
  + Disclaimer, lá»i cáº£m Æ¡n sponsor cá»¥ thá»ƒ
  + Báº¥t ká»³ thÃ´ng tin nháº­n diá»‡n thÆ°Æ¡ng hiá»‡u nÃ o
- KHÃ”NG COPY ná»™i dung cÃ¡ nhÃ¢n hÃ³a tá»« máº«u vÃ o description má»›i

KIáº¾N TRÃšC DESCRIPTION 4 Táº¦NG (Timestamps sáº½ Ä‘Æ°á»£c tÃ­nh chÃ­nh xÃ¡c tá»« voice duration vÃ  inject sau):

Táº¦NG 1 â€” THE HOOK (1-2 dÃ²ng Ä‘áº§u â€” "Khu Vá»±c VÃ ng"):
- Pháº§n DUY NHáº¤T hiá»ƒn thá»‹ trÆ°á»›c nÃºt "Xem thÃªm"
- Chá»©a tá»« khÃ³a chÃ­nh trong 150 kÃ½ tá»± Ä‘áº§u
- Äi tháº³ng vÃ o váº¥n Ä‘á», KHÃ”NG viáº¿t "ChÃ o cÃ¡c báº¡n, hÃ´m nay mÃ¬nh sáº½..."
- CÃ¢u hook gÃ¢y tÃ² mÃ² + lá»i kÃªu gá»i háº¥p dáº«n

Táº¦NG 2 â€” Ná»˜I DUNG CHI TIáº¾T (Mini Blog):
- 150-300 tá»« tÃ³m táº¯t cÃ¡c Ä‘iá»ƒm chÃ­nh cá»§a video
- NgÃ´n ngá»¯ tá»± nhiÃªn, lá»“ng ghÃ©p LSI keywords khÃ©o lÃ©o
- KHÃ”NG nhá»“i nháº¿t tá»« khÃ³a, KHÃ”NG liá»‡t kÃª tá»« khÃ³a vÃ´ nghÄ©a

[TIMESTAMPS_PLACEHOLDER]
(Pháº§n timestamps sáº½ Ä‘Æ°á»£c há»‡ thá»‘ng tá»± Ä‘á»™ng tÃ­nh toÃ¡n vÃ  chÃ¨n vÃ o Ä‘Ã¢y sau khi voice Ä‘Æ°á»£c táº¡o xong)

Táº¦NG 3 â€” CTA & LINKS:
{f"Sá»¬ Dá»¤NG CHÃNH XÃC pháº§n CTA sau Ä‘Ã¢y do user cung cáº¥p (KHÃ”NG thay Ä‘á»•i ná»™i dung, chá»‰ format phÃ¹ há»£p):{chr(10)}{request.custom_cta}" if request.custom_cta else "- ÄÄƒng kÃ½ kÃªnh: [Link Ä‘Äƒng kÃ½]{chr(10)}- Video liÃªn quan / Playlist cÃ¹ng chá»§ Ä‘á»{chr(10)}- Link tÃ i nguyÃªn náº¿u phÃ¹ há»£p"}

Táº¦NG 4 â€” HASHTAGS:
- ChÃ­nh xÃ¡c 3-5 hashtags, Ä‘áº·t á»Ÿ cuá»‘i cÃ¹ng
- Cáº¥u trÃºc: 1 hashtag thÆ°Æ¡ng hiá»‡u + 1 hashtag chá»§ Ä‘á» rá»™ng + 1-3 hashtag ngÃ¡ch háº¹p
- KHÃ”NG dÃ¹ng quÃ¡ 15 hashtags (YouTube sáº½ vÃ´ hiá»‡u hÃ³a táº¥t cáº£)
{style_context}
{desc_samples_text}

Ká»ŠCH Báº¢N:
{script_preview}

TRáº¢ Vá»€ DESCRIPTION HOÃ€N CHá»ˆNH theo Ä‘Ãºng 4 táº§ng trÃªn (Bá» QUA [TIMESTAMPS_PLACEHOLDER] â€” há»‡ thá»‘ng sáº½ tá»± chÃ¨n timestamps). Viáº¿t báº±ng tiáº¿ng Viá»‡t (trá»« khi ká»‹ch báº£n báº±ng ngÃ´n ngá»¯ khÃ¡c)."""

            try:
                desc_result = await asyncio.to_thread(
                    ai_client.generate, desc_prompt, temperature=0.7
                )
                results["description"] = desc_result.strip()
                logger.info(f"[YT-Meta] Description generated: {len(results['description'])} chars")
            except Exception as e:
                logger.error(f"[YT-Meta] Description generation failed: {e}")
                results["description"] = f"[Lá»—i táº¡o description: {str(e)[:100]}]"
        
        # â”€â”€ Generate Thumbnail Prompt â”€â”€
        if request.generate_thumbnail_prompt:
            thumb_prompt = f"""Báº¡n lÃ  chuyÃªn gia thiáº¿t káº¿ thumbnail YouTube. HÃ£y táº¡o 1 prompt chi tiáº¿t Ä‘á»ƒ AI táº¡o thumbnail háº¥p dáº«n cho video.

CÃ”NG THá»¨C 7 THÃ€NH PHáº¦N THUMBNAIL:
1. Subject/Focal Point â€” nhÃ¢n váº­t hoáº·c Ä‘á»‘i tÆ°á»£ng chÃ­nh
2. Expression/Emotion â€” biá»ƒu cáº£m máº¡nh (shock, vui, sá»£...)
3. Background/Setting â€” bá»‘i cáº£nh phÃ¹ há»£p ná»™i dung
4. Color Scheme â€” mÃ u sáº¯c tÆ°Æ¡ng pháº£n cao (complementary colors)
5. Text Overlay â€” 2-4 tá»« lá»›n, bold, dá»… Ä‘á»c trÃªn mobile
6. Composition â€” Rule of thirds, focal point rÃµ rÃ ng
7. Style â€” Professional, cinematic, high contrast

YÃŠU Cáº¦U:
- Prompt báº±ng tiáº¿ng Anh (Ä‘á»ƒ dÃ¹ng vá»›i AI image generator)
- MÃ´ táº£ chi tiáº¿t, cá»¥ thá»ƒ
- Tá»‘i Æ°u cho 1280x720 (16:9)
- Pháº£i báº¯t máº¯t á»Ÿ kÃ­ch thÆ°á»›c nhá» (mobile)
- Include text overlay suggestion
{style_context}
"""
            # Inject sync analysis into thumbnail prompt
            if request.sync_analysis:
                sa = request.sync_analysis
                sync_thumb_parts = []
                if sa.get('characters'):
                    chars = ", ".join([c.get('name', '') for c in sa['characters'][:3]])
                    sync_thumb_parts.append(f"Key characters to feature: {chars}")
                if sa.get('visual_style'):
                    sync_thumb_parts.append(f"Visual style reference: {sa['visual_style']}")
                if sa.get('settings'):
                    sets = ", ".join([s.get('name', '') for s in sa['settings'][:3]])
                    sync_thumb_parts.append(f"Setting references: {sets}")
                if sync_thumb_parts:
                    thumb_prompt += "\n\nSYNC ANALYSIS CONTEXT (use for visual consistency):\n" + "\n".join([f"- {p}" for p in sync_thumb_parts]) + "\n"

            thumb_prompt += f"""Ká»ŠÌ£CH Báº¢N:
{script_preview}

TRáº¢ Vá»€ ÄÃšNG 1 THUMBNAIL PROMPT báº±ng tiáº¿ng Anh, KHÃ”NG giáº£i thÃ­ch thÃªm."""

            try:
                thumb_result = await asyncio.to_thread(
                    ai_client.generate, thumb_prompt, temperature=0.8
                )
                results["thumbnail_prompt"] = thumb_result.strip()
                logger.info(f"[YT-Meta] Thumbnail prompt generated: {len(results['thumbnail_prompt'])} chars")
            except Exception as e:
                logger.error(f"[YT-Meta] Thumbnail prompt generation failed: {e}")
                results["thumbnail_prompt"] = f"[Lá»—i táº¡o thumbnail prompt: {str(e)[:100]}]"

        return results

    except Exception as e:
        import traceback
        logger.error(f"[YT-Meta] Error: {e}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


# ===== Metadata Style Analysis (Title / Description / Thumbnail) =====

class AnalyzeMetadataStylesRequest(BaseModel):
    title_samples: List[str] = []          # 5-20 sample YouTube titles
    description_samples: List[str] = []    # 5-20 sample YouTube descriptions
    thumbnail_descriptions: List[str] = [] # Text descriptions of thumbnail visuals (legacy)
    thumbnail_images: List[str] = []       # Base64 data URIs of thumbnail images for Vision API
    model: Optional[str] = None
    output_language: str = ""  # Target output language - if set and differs from input, title/desc will be translated first


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HELPER: Parse JSON from AI response (strips markdown fences, etc.)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _parse_metadata_json(text: str) -> dict:
    """Parse JSON from AI response, handling markdown code fences."""
    cleaned = text.strip()
    if cleaned.startswith("```"):
        cleaned = cleaned.split("\n", 1)[1] if "\n" in cleaned else cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        cleaned = cleaned.strip()
    try:
        return json_module.loads(cleaned)
    except Exception:
        # Try to extract JSON from the response
        import re
        json_match = re.search(r'\{[\s\S]*\}', cleaned)
        if json_match:
            try:
                return json_module.loads(json_match.group())
            except Exception:
                pass
        return None


@router.post("/analyze-metadata-styles-stream")
async def analyze_metadata_styles_stream(request: AnalyzeMetadataStylesRequest):
    """
    SSE streaming endpoint to analyze title, description, and thumbnail styles in parallel.
    
    ğŸ”„ CONVERSATION-BASED ANALYSIS (same pattern as StyleA voice analysis):
    STEP 1: Create conversation with domain-specific system prompt
    STEP 2: Analyze each sample individually in conversation (AI builds memory)
    STEP 3: Send synthesis prompt to produce final profile from accumulated memory
    STEP 4: Parse and return structured JSON profile
    
    All 3 analysis types run in parallel via ThreadPoolExecutor.
    Falls back to single-shot when conversation support is unavailable.
    """
    has_titles = len(request.title_samples) > 0
    has_descriptions = len(request.description_samples) > 0
    has_thumbnails = len(request.thumbnail_images) > 0 or len(request.thumbnail_descriptions) > 0

    if not has_titles and not has_descriptions and not has_thumbnails:
        raise HTTPException(status_code=400, detail="Cáº§n Ã­t nháº¥t 1 loáº¡i máº«u (title/description/thumbnail)")

    ai_client = get_configured_ai_client(request.model)
    supports_conversation = ai_client.has_conversation_support()

    async def stream_analysis():
        import concurrent.futures
        import queue as queue_module

        results = {
            "title_style": None,
            "description_style": None,
            "thumbnail_style": None,
        }
        total_tasks = sum([has_titles, has_descriptions, has_thumbnails])

        # Thread-safe progress queue for real-time updates from workers
        progress_queue = queue_module.Queue()

        def send_event(event_type: str, data: dict) -> str:
            return f"event: {event_type}\ndata: {json_module.dumps(data, ensure_ascii=False)}\n\n"

        def report_progress(step: str, percentage: int, message: str):
            """Thread-safe progress reporting from worker threads."""
            progress_queue.put({"step": step, "percentage": percentage, "message": message})

        yield send_event("progress", {
            "step": "init", "percentage": 0,
            "message": f"Báº¯t Ä‘áº§u phÃ¢n tÃ­ch {total_tasks} má»¥c {'(conversation mode)' if supports_conversation else '(single-shot mode)'}..."
        })

        # â”€â”€ PRE-TRANSLATION: Translate title/description samples if output language differs â”€â”€
        translated_titles = list(request.title_samples)
        translated_descs = list(request.description_samples)
        output_lang = request.output_language.strip()
        
        if output_lang and (has_titles or has_descriptions):
            # Auto-detect input language from first available sample
            detect_text = (request.title_samples[0] if has_titles else request.description_samples[0])
            detected_input = _detect_language(detect_text)
            
            if detected_input != output_lang:
                lang_names = {"en": "English", "vi": "Vietnamese", "ja": "Japanese", "ko": "Korean", "zh": "Chinese", "es": "Spanish", "fr": "French", "th": "Thai", "de": "German", "pt": "Portuguese", "ru": "Russian"}
                src_name = lang_names.get(detected_input, detected_input)
                out_name = lang_names.get(output_lang, output_lang)
                
                loop = asyncio.get_event_loop()
                
                # Translate titles
                if has_titles:
                    yield send_event("progress", {"step": "translate_title", "percentage": 1, "message": f"Äang dá»‹ch {len(translated_titles)} tiÃªu Ä‘á» {src_name} â†’ {out_name}..."})
                    new_titles = []
                    for i, title in enumerate(translated_titles):
                        try:
                            prompt = f"Translate this YouTube title from {src_name} to {out_name}. Output ONLY the translated title, nothing else:\n\n{title}"
                            result = await loop.run_in_executor(None, lambda p=prompt: ai_client.generate(p, temperature=0.3))
                            new_titles.append(result.strip() if result and len(result.strip()) > 3 else title)
                        except Exception:
                            new_titles.append(title)
                    translated_titles = new_titles
                    yield send_event("progress", {"step": "translate_title_done", "percentage": 3, "message": f"ÄÃ£ dá»‹ch {len(new_titles)} tiÃªu Ä‘á» sang {out_name}"})
                
                # Translate descriptions
                if has_descriptions:
                    yield send_event("progress", {"step": "translate_desc", "percentage": 3, "message": f"Äang dá»‹ch {len(translated_descs)} mÃ´ táº£ {src_name} â†’ {out_name}..."})
                    new_descs = []
                    for i, desc in enumerate(translated_descs):
                        try:
                            prompt = f"""Translate this YouTube description from {src_name} to {out_name}.
Rules: Preserve structure, line breaks, links. Output ONLY the translated text.\n\n{desc}"""
                            result = await loop.run_in_executor(None, lambda p=prompt: ai_client.generate(p, temperature=0.3))
                            new_descs.append(result.strip() if result and len(result.strip()) > 10 else desc)
                        except Exception:
                            new_descs.append(desc)
                    translated_descs = new_descs
                    yield send_event("progress", {"step": "translate_desc_done", "percentage": 5, "message": f"ÄÃ£ dá»‹ch {len(new_descs)} mÃ´ táº£ sang {out_name}"})
                
                # Update request samples with translated versions
                request.title_samples = translated_titles
                request.description_samples = translated_descs

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # TITLE ANALYSIS â€” Conversation-based
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        def analyze_titles() -> dict:
            samples = request.title_samples[:20]
            num_samples = len(samples)
            
            if supports_conversation:
                # â”€â”€ STEP 1: Create conversation â”€â”€
                system_prompt = """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch tiÃªu Ä‘á» YouTube vá»›i kinh nghiá»‡m SEO vÃ  CTR optimization.

NHIá»†M Vá»¤: PhÃ¢n tÃ­ch Tá»ªNG tiÃªu Ä‘á» Ä‘Æ°á»£c gá»­i Ä‘áº¿n vÃ  GHI NHá»š táº¥t cáº£ Ä‘áº·c Ä‘iá»ƒm Ä‘á»ƒ cuá»‘i cÃ¹ng tá»•ng há»£p thÃ nh "PHONG CÃCH Äáº¶T TIÃŠU Äá»€".

QUY Táº®C:
1. Má»—i tiÃªu Ä‘á»: Tráº£ vá» phÃ¢n tÃ­ch ngáº¯n gá»n dáº¡ng JSON
2. GHI NHá»š: Táº¥t cáº£ patterns, hooks, power words, cáº¥u trÃºc qua tá»«ng tiÃªu Ä‘á»
3. Khi Ä‘Æ°á»£c yÃªu cáº§u Tá»”NG Há»¢P: ÄÃºc káº¿t tá»« Táº¤T Cáº¢ tiÃªu Ä‘á» Ä‘Ã£ phÃ¢n tÃ­ch
4. Output PHáº¢I viáº¿t báº±ng tiáº¿ng Viá»‡t CÃ“ Dáº¤U

HÃ£y sáºµn sÃ ng phÃ¢n tÃ­ch!"""
                
                try:
                    conv_id = ai_client.start_conversation(system_prompt)
                    logger.info(f"[Title] Conversation started: {conv_id[:8]}...")
                    report_progress("title_start", 5, "ğŸ·ï¸ Báº¯t Ä‘áº§u phÃ¢n tÃ­ch tiÃªu Ä‘á»...")
                except Exception as e:
                    logger.error(f"[Title] Failed to start conversation: {e}, falling back to single-shot")
                    return _analyze_titles_single_shot(ai_client, samples)
                
                # â”€â”€ STEP 2: Analyze each title individually â”€â”€
                individual_results = []
                for i, title in enumerate(samples, 1):
                    report_progress("title_sample", 5 + int((i / num_samples) * 25), f"ğŸ·ï¸ PhÃ¢n tÃ­ch tiÃªu Ä‘á» {i}/{num_samples}...")
                    analysis_prompt = f"""PhÃ¢n tÃ­ch TIÃŠU Äá»€ #{i}/{num_samples}:

"{title}"

Tráº£ vá» JSON ngáº¯n gá»n:
{{
    "hook_type": "curiosity_gap/negativity_bias/authority/specificity/how_to/listicle/social_proof",
    "power_words": ["tá»« máº¡nh 1", "tá»« máº¡nh 2"],
    "emotional_trigger": "cáº£m xÃºc kÃ­ch hoáº¡t",
    "structure": "mÃ´ táº£ cáº¥u trÃºc ngáº¯n",
    "length": {len(title)},
    "effectiveness": "1-10 vÃ  lÃ½ do ngáº¯n"
}}"""
                    try:
                        response = ai_client.send_message(conv_id, analysis_prompt, temperature=0.3)
                        parsed = _parse_metadata_json(response)
                        if parsed:
                            parsed['title'] = title
                            parsed['index'] = i
                            individual_results.append(parsed)
                        logger.info(f"[Title] âœ… Analyzed {i}/{num_samples}")
                    except Exception as e:
                        logger.error(f"[Title] âŒ Error analyzing title {i}: {e}")
                
                report_progress("title_synth", 30, "ğŸ·ï¸ Tá»•ng há»£p phong cÃ¡ch tiÃªu Ä‘á»...")
                # â”€â”€ STEP 3: Synthesize â”€â”€
                synthesis_prompt = f"""BÃ¢y giá», dá»±a trÃªn Táº¤T Cáº¢ {num_samples} tiÃªu Ä‘á» báº¡n vá»«a phÃ¢n tÃ­ch, hÃ£y ÄÃšC Káº¾T thÃ nh "PHONG CÃCH Äáº¶T TIÃŠU Äá»€".

âš ï¸ QUY Táº®C:
1. KHÃ”NG nháº¯c Ä‘áº¿n ná»™i dung cá»¥ thá»ƒ cá»§a báº¥t ká»³ tiÃªu Ä‘á» nÃ o
2. CHá»ˆ mÃ´ táº£ PATTERNS vÃ  TECHNIQUES chung xuáº¥t hiá»‡n xuyÃªn suá»‘t {num_samples} tiÃªu Ä‘á»
3. Má»—i trÆ°á»ng pháº£i lÃ  MÃ” Táº¢ Tá»”NG QUÃT Ã¡p dá»¥ng Ä‘Æ°á»£c cho táº¥t cáº£ tiÃªu Ä‘á»

TRáº¢ Vá»€ JSON (khÃ´ng giáº£i thÃ­ch thÃªm):
{{
  "hook_patterns": ["pattern mÃ´ táº£ 1", "pattern 2"],
  "dominant_hooks": ["loáº¡i hook chá»§ Ä‘áº¡o 1", "loáº¡i 2"],
  "avg_length": 45,
  "syntax_patterns": {{
    "front_loading": true hoáº·c false,
    "separator": "|" hoáº·c "-" hoáº·c ":" hoáº·c "none",
    "capitalization": "title_case" hoáº·c "strategic_caps" hoáº·c "all_caps"
  }},
  "power_words": ["tá»«1", "tá»«2", "tá»«3"],
  "emotional_tone": "mÃ´ táº£ tÃ´ng cáº£m xÃºc",
  "title_formulas": ["cÃ´ng thá»©c 1", "cÃ´ng thá»©c 2"],
  "style_summary": "TÃ³m táº¯t 2-3 cÃ¢u vá» phong cÃ¡ch Ä‘áº·t tiÃªu Ä‘á»"
}}"""
                try:
                    response = ai_client.send_message(conv_id, synthesis_prompt, temperature=0.3)
                    parsed = _parse_metadata_json(response)
                    if parsed:
                        logger.info(f"[Title] âœ… Synthesis complete")
                        return parsed
                except Exception as e:
                    logger.error(f"[Title] Synthesis failed: {e}")
                
                # Fallback if synthesis fails
                return {"style_summary": f"PhÃ¢n tÃ­ch {num_samples} tiÃªu Ä‘á» thÃ nh cÃ´ng nhÆ°ng tá»•ng há»£p tháº¥t báº¡i", "individual_count": len(individual_results)}
            else:
                return _analyze_titles_single_shot(ai_client, samples)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # DESCRIPTION ANALYSIS â€” Conversation-based
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        def analyze_descriptions() -> dict:
            samples = request.description_samples[:20]
            num_samples = len(samples)
            
            if supports_conversation:
                # â”€â”€ STEP 1: Create conversation â”€â”€
                system_prompt = """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch mÃ´ táº£ video YouTube, chuyÃªn vá» SEO vÃ  audience engagement.

NHIá»†M Vá»¤: PhÃ¢n tÃ­ch Tá»ªNG mÃ´ táº£ video Ä‘Æ°á»£c gá»­i Ä‘áº¿n vÃ  GHI NHá»š táº¥t cáº£ Ä‘áº·c Ä‘iá»ƒm Ä‘á»ƒ cuá»‘i cÃ¹ng tá»•ng há»£p thÃ nh "PHONG CÃCH MÃ” Táº¢".

QUY Táº®C:
1. Má»—i mÃ´ táº£: Tráº£ vá» phÃ¢n tÃ­ch ngáº¯n gá»n dáº¡ng JSON
2. GHI NHá»š: Cáº¥u trÃºc, hooks, CTA, hashtags, brand voice qua tá»«ng mÃ´ táº£
3. Khi Ä‘Æ°á»£c yÃªu cáº§u Tá»”NG Há»¢P: ÄÃºc káº¿t tá»« Táº¤T Cáº¢ mÃ´ táº£ Ä‘Ã£ phÃ¢n tÃ­ch
4. Output PHáº¢I viáº¿t báº±ng tiáº¿ng Viá»‡t CÃ“ Dáº¤U

HÃ£y sáºµn sÃ ng phÃ¢n tÃ­ch!"""
                
                try:
                    conv_id = ai_client.start_conversation(system_prompt)
                    logger.info(f"[Description] Conversation started: {conv_id[:8]}...")
                    report_progress("desc_start", 35, "ğŸ“ Báº¯t Ä‘áº§u phÃ¢n tÃ­ch mÃ´ táº£...")
                except Exception as e:
                    logger.error(f"[Description] Failed to start conversation: {e}, falling back to single-shot")
                    return _analyze_descriptions_single_shot(ai_client, samples)
                
                # â”€â”€ STEP 2: Analyze each description individually â”€â”€
                individual_results = []
                for i, desc in enumerate(samples, 1):
                    report_progress("desc_sample", 35 + int((i / num_samples) * 25), f"ğŸ“ PhÃ¢n tÃ­ch mÃ´ táº£ {i}/{num_samples}...")
                    # Truncate very long descriptions to avoid token limits
                    desc_content = desc[:3000] if len(desc) > 3000 else desc
                    
                    analysis_prompt = f"""PhÃ¢n tÃ­ch MÃ” Táº¢ #{i}/{num_samples}:

\"\"\"
{desc_content}
\"\"\"

Tráº£ vá» JSON ngáº¯n gá»n:
{{
    "hook_line": "dÃ²ng hook Ä‘áº§u tiÃªn",
    "word_count": {len(desc.split())},
    "has_timestamps": true/false,
    "has_cta": true/false,
    "cta_type": "subscribe/like/link/none",
    "hashtag_count": 0,
    "tone": "thÃ¢n máº­t/trang trá»ng/hÃ i hÆ°á»›c",
    "structure": "mÃ´ táº£ cáº¥u trÃºc ngáº¯n",
    "key_elements": ["yáº¿u tá»‘ 1", "yáº¿u tá»‘ 2"]
}}"""
                    try:
                        response = ai_client.send_message(conv_id, analysis_prompt, temperature=0.3)
                        parsed = _parse_metadata_json(response)
                        if parsed:
                            parsed['index'] = i
                            individual_results.append(parsed)
                        logger.info(f"[Description] âœ… Analyzed {i}/{num_samples}")
                    except Exception as e:
                        logger.error(f"[Description] âŒ Error analyzing description {i}: {e}")
                
                # â”€â”€ STEP 3: Synthesize â”€â”€
                synthesis_prompt = f"""BÃ¢y giá», dá»±a trÃªn Táº¤T Cáº¢ {num_samples} mÃ´ táº£ video báº¡n vá»«a phÃ¢n tÃ­ch, hÃ£y ÄÃšC Káº¾T thÃ nh "PHONG CÃCH MÃ” Táº¢".

âš ï¸ QUY Táº®C:
1. KHÃ”NG nháº¯c Ä‘áº¿n ná»™i dung cá»¥ thá»ƒ cá»§a báº¥t ká»³ mÃ´ táº£ nÃ o
2. CHá»ˆ mÃ´ táº£ PATTERNS vÃ  TECHNIQUES chung
3. Má»—i trÆ°á»ng pháº£i lÃ  MÃ” Táº¢ Tá»”NG QUÃT

PHÃ‚N TÃCH KIáº¾N TRÃšC MÃ” Táº¢ 5 Táº¦NG dá»±a trÃªn patterns Ä‘Ã£ tháº¥y.

TRáº¢ Vá»€ JSON (khÃ´ng giáº£i thÃ­ch thÃªm):
{{
  "hook_style": "mÃ´ táº£ cÃ¡ch viáº¿t hook (2 dÃ²ng Ä‘áº§u)",
  "body_style": {{
    "avg_word_count": 200,
    "writing_approach": "tÃ³m táº¯t/ká»ƒ chuyá»‡n/liá»‡t kÃª",
    "uses_lsi_keywords": true/false
  }},
  "uses_timestamps": true/false,
  "timestamp_style": "mÃ´ táº£ phong cÃ¡ch timestamps náº¿u cÃ³",
  "cta_patterns": ["CTA pattern 1", "CTA pattern 2"],
  "cta_position": "top/middle/bottom/distributed",
  "hashtag_strategy": {{
    "avg_count": 3,
    "types": ["brand", "topic", "niche"]
  }},
  "brand_voice": {{
    "tone": "thÃ¢n máº­t/trang trá»ng/hÃ i hÆ°á»›c",
    "vocabulary": ["tá»« Ä‘áº·c trÆ°ng 1", "tá»« 2"],
    "addressing": "cÃ¡ch xÆ°ng hÃ´"
  }},
  "structure_template": "MÃ´ táº£ cáº¥u trÃºc máº«u cho description má»›i (template)",
  "style_summary": "TÃ³m táº¯t 2-3 cÃ¢u vá» phong cÃ¡ch mÃ´ táº£"
}}"""
                try:
                    response = ai_client.send_message(conv_id, synthesis_prompt, temperature=0.3)
                    parsed = _parse_metadata_json(response)
                    if parsed:
                        logger.info(f"[Description] âœ… Synthesis complete")
                        return parsed
                except Exception as e:
                    logger.error(f"[Description] Synthesis failed: {e}")
                
                return {"style_summary": f"PhÃ¢n tÃ­ch {num_samples} mÃ´ táº£ thÃ nh cÃ´ng nhÆ°ng tá»•ng há»£p tháº¥t báº¡i", "individual_count": len(individual_results)}
            else:
                return _analyze_descriptions_single_shot(ai_client, samples)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # THUMBNAIL ANALYSIS â€” Conversation-based
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        def analyze_thumbnails() -> dict:
            thumb_images = request.thumbnail_images[:10]
            thumb_descs = request.thumbnail_descriptions[:10]
            num_images = len(thumb_images)
            num_samples = max(num_images, len(thumb_descs))

            # â”€â”€ VISION-BASED ANALYSIS (when real images are provided) â”€â”€
            if num_images > 0:
                report_progress("thumb_start", 65, "ğŸ–¼ï¸ PhÃ¢n tÃ­ch thumbnail báº±ng Vision AI...")
                logger.info(f"[Thumbnail] Vision analysis with {num_images} images")

                vision_prompt = f"""Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch thumbnail YouTube, chuyÃªn vá» visual design vÃ  CTR optimization.

PHÃ‚N TÃCH {num_images} THUMBNAIL ÄÃNH KÃˆM vÃ  Ä‘Ãºc káº¿t thÃ nh "PHONG CÃCH THUMBNAIL".

CHO Má»–I THUMBNAIL, phÃ¢n tÃ­ch:
- Bá»‘ cá»¥c (layout, rule of thirds, subject position)
- MÃ u sáº¯c (dominant colors, contrast, saturation)
- Typography (text overlay, font style, word count)
- Cáº£m xÃºc (facial expressions, emotion, eye contact)
- Visual storytelling (before/after, curiosity gap, arrows)

âš ï¸ QUY Táº®C:
1. KHÃ”NG nháº¯c Ä‘áº¿n ná»™i dung cá»¥ thá»ƒ cá»§a báº¥t ká»³ thumbnail nÃ o
2. CHá»ˆ mÃ´ táº£ PATTERNS vÃ  DESIGN PRINCIPLES chung xuáº¥t hiá»‡n xuyÃªn suá»‘t
3. Má»—i trÆ°á»ng pháº£i lÃ  MÃ” Táº¢ Tá»”NG QUÃT Ã¡p dá»¥ng Ä‘Æ°á»£c cho táº¥t cáº£ thumbnail
4. Output PHáº¢I viáº¿t báº±ng tiáº¿ng Viá»‡t CÃ“ Dáº¤U

TRáº¢ Vá»€ JSON (khÃ´ng giáº£i thÃ­ch thÃªm):
{{
  "composition": {{
    "layout": "rule_of_thirds/centered/asymmetric",
    "subject_position": "left/right/center",
    "visual_hierarchy": "mÃ´ táº£ phÃ¢n cáº¥p thá»‹ giÃ¡c"
  }},
  "color_scheme": {{
    "dominant_colors": ["mÃ u chá»§ Ä‘áº¡o 1", "mÃ u 2"],
    "contrast_style": "high/medium/low",
    "saturation": "hyper/normal/muted",
    "emotional_mapping": "mÃ´ táº£ cáº£m xÃºc tá»« mÃ u sáº¯c"
  }},
  "typography": {{
    "avg_word_count": 3,
    "font_style": "bold_sans_serif/custom/minimal",
    "effects": ["stroke", "shadow"]
  }},
  "emotional_signals": {{
    "uses_faces": true/false,
    "dominant_emotion": "shock/joy/curiosity/fear",
    "eye_contact": true/false,
    "exaggeration_level": "high/medium/low"
  }},
  "storytelling_techniques": ["before_after", "curiosity_gap", "arrows"],
  "overall_style": "professional/casual/cinematic",
  "text_strategy": "complementary/repetitive/curiosity_gap",
  "thumbnail_formula": "CÃ´ng thá»©c thumbnail Ä‘áº·c trÆ°ng nháº¥t",
  "style_summary": "TÃ³m táº¯t 2-3 cÃ¢u vá» phong cÃ¡ch thumbnail"
}}"""
                try:
                    report_progress("thumb_sample", 75, f"ğŸ–¼ï¸ Gá»­i {num_images} áº£nh Ä‘áº¿n Vision AI...")
                    response = ai_client.generate_with_images(
                        vision_prompt, thumb_images, temperature=0.3, max_tokens=4000
                    )
                    parsed = _parse_metadata_json(response)
                    if parsed:
                        logger.info(f"[Thumbnail] âœ… Vision analysis complete")
                        report_progress("thumb_done", 90, "HoÃ n thÃ nh phÃ¢n tÃ­ch thumbnail âœ“")
                        return parsed
                    else:
                        logger.warning(f"[Thumbnail] Vision response not valid JSON, raw: {response[:300]}")
                        return {"style_summary": response.strip()[:500], "raw": True}
                except Exception as e:
                    logger.error(f"[Thumbnail] Vision analysis failed: {e}, falling back to text-based")
                    # Fall through to conversation-based analysis with descriptions
                    if not thumb_descs:
                        return {"style_summary": f"Vision analysis failed: {str(e)[:200]}", "error": True}

            # â”€â”€ TEXT-BASED ANALYSIS (fallback: when only descriptions are available) â”€â”€
            samples = thumb_descs
            num_samples = len(samples)
            if num_samples == 0:
                return {"style_summary": "KhÃ´ng cÃ³ dá»¯ liá»‡u thumbnail Ä‘á»ƒ phÃ¢n tÃ­ch", "error": True}

            if supports_conversation:
                # â”€â”€ STEP 1: Create conversation â”€â”€
                system_prompt = """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch thumbnail YouTube, chuyÃªn vá» visual design vÃ  CTR optimization.

NHIá»†M Vá»¤: PhÃ¢n tÃ­ch Tá»ªNG mÃ´ táº£ thumbnail Ä‘Æ°á»£c gá»­i Ä‘áº¿n vÃ  GHI NHá»š táº¥t cáº£ Ä‘áº·c Ä‘iá»ƒm Ä‘á»ƒ cuá»‘i cÃ¹ng tá»•ng há»£p thÃ nh "PHONG CÃCH THUMBNAIL".

QUY Táº®C:
1. Má»—i thumbnail: Tráº£ vá» phÃ¢n tÃ­ch ngáº¯n gá»n dáº¡ng JSON
2. GHI NHá»š: Bá»‘ cá»¥c, mÃ u sáº¯c, cáº£m xÃºc, typography, storytelling qua tá»«ng thumbnail
3. Khi Ä‘Æ°á»£c yÃªu cáº§u Tá»”NG Há»¢P: ÄÃºc káº¿t tá»« Táº¤T Cáº¢ thumbnail Ä‘Ã£ phÃ¢n tÃ­ch
4. Output PHáº¢I viáº¿t báº±ng tiáº¿ng Viá»‡t CÃ“ Dáº¤U

HÃ£y sáºµn sÃ ng phÃ¢n tÃ­ch!"""
                
                try:
                    conv_id = ai_client.start_conversation(system_prompt)
                    logger.info(f"[Thumbnail] Conversation started: {conv_id[:8]}...")
                    report_progress("thumb_start", 65, "ğŸ–¼ï¸ Báº¯t Ä‘áº§u phÃ¢n tÃ­ch thumbnail (text)...")
                except Exception as e:
                    logger.error(f"[Thumbnail] Failed to start conversation: {e}, falling back to single-shot")
                    return _analyze_thumbnails_single_shot(ai_client, samples)
                
                # â”€â”€ STEP 2: Analyze each thumbnail individually â”€â”€
                individual_results = []
                for i, thumb_desc in enumerate(samples, 1):
                    report_progress("thumb_sample", 65 + int((i / num_samples) * 25), f"ğŸ–¼ï¸ PhÃ¢n tÃ­ch thumbnail {i}/{num_samples}...")
                    analysis_prompt = f"""PhÃ¢n tÃ­ch THUMBNAIL #{i}/{num_samples}:

MÃ´ táº£:
\"\"\"{thumb_desc}\"\"\"

Tráº£ vá» JSON ngáº¯n gá»n:
{{
    "layout": "rule_of_thirds/centered/asymmetric",
    "dominant_colors": ["mÃ u 1", "mÃ u 2"],
    "has_text": true/false,
    "text_words": 0,
    "has_face": true/false,
    "emotion": "shock/joy/curiosity/fear/neutral",
    "contrast": "high/medium/low",
    "storytelling": "before_after/curiosity_gap/reveal/arrows/none",
    "overall_feel": "mÃ´ táº£ ngáº¯n"
}}"""
                    try:
                        response = ai_client.send_message(conv_id, analysis_prompt, temperature=0.3)
                        parsed = _parse_metadata_json(response)
                        if parsed:
                            parsed['index'] = i
                            individual_results.append(parsed)
                        logger.info(f"[Thumbnail] âœ… Analyzed {i}/{num_samples}")
                    except Exception as e:
                        logger.error(f"[Thumbnail] âŒ Error analyzing thumbnail {i}: {e}")
                
                # â”€â”€ STEP 3: Synthesize â”€â”€
                synthesis_prompt = f"""BÃ¢y giá», dá»±a trÃªn Táº¤T Cáº¢ {num_samples} thumbnail báº¡n vá»«a phÃ¢n tÃ­ch, hÃ£y ÄÃšC Káº¾T thÃ nh "PHONG CÃCH THUMBNAIL".

âš ï¸ QUY Táº®C:
1. KHÃ”NG nháº¯c Ä‘áº¿n ná»™i dung cá»¥ thá»ƒ cá»§a báº¥t ká»³ thumbnail nÃ o
2. CHá»ˆ mÃ´ táº£ PATTERNS vÃ  DESIGN PRINCIPLES chung
3. Má»—i trÆ°á»ng pháº£i lÃ  MÃ” Táº¢ Tá»”NG QUÃT

PHÃ‚N TÃCH PHÃP Y HÃŒNH áº¢NH: Bá»‘ cá»¥c, MÃ u sáº¯c, Typography, Cáº£m xÃºc, Visual Storytelling, Phong cÃ¡ch tá»•ng thá»ƒ, Text Strategy.

TRáº¢ Vá»€ JSON (khÃ´ng giáº£i thÃ­ch thÃªm):
{{
  "composition": {{
    "layout": "rule_of_thirds/centered/asymmetric",
    "subject_position": "left/right/center",
    "visual_hierarchy": "mÃ´ táº£ phÃ¢n cáº¥p thá»‹ giÃ¡c"
  }},
  "color_scheme": {{
    "dominant_colors": ["mÃ u chá»§ Ä‘áº¡o 1", "mÃ u 2"],
    "contrast_style": "high/medium/low",
    "saturation": "hyper/normal/muted",
    "emotional_mapping": "mÃ´ táº£ cáº£m xÃºc tá»« mÃ u sáº¯c"
  }},
  "typography": {{
    "avg_word_count": 3,
    "font_style": "bold_sans_serif/custom/minimal",
    "effects": ["stroke", "shadow"]
  }},
  "emotional_signals": {{
    "uses_faces": true/false,
    "dominant_emotion": "shock/joy/curiosity/fear",
    "eye_contact": true/false,
    "exaggeration_level": "high/medium/low"
  }},
  "storytelling_techniques": ["before_after", "curiosity_gap", "arrows"],
  "overall_style": "professional/casual/cinematic",
  "text_strategy": "complementary/repetitive/curiosity_gap",
  "thumbnail_formula": "CÃ´ng thá»©c thumbnail Ä‘áº·c trÆ°ng nháº¥t",
  "style_summary": "TÃ³m táº¯t 2-3 cÃ¢u vá» phong cÃ¡ch thumbnail"
}}"""
                try:
                    response = ai_client.send_message(conv_id, synthesis_prompt, temperature=0.3)
                    parsed = _parse_metadata_json(response)
                    if parsed:
                        logger.info(f"[Thumbnail] âœ… Synthesis complete")
                        return parsed
                except Exception as e:
                    logger.error(f"[Thumbnail] Synthesis failed: {e}")
                
                return {"style_summary": f"PhÃ¢n tÃ­ch {num_samples} thumbnail thÃ nh cÃ´ng nhÆ°ng tá»•ng há»£p tháº¥t báº¡i", "individual_count": len(individual_results)}
            else:
                return _analyze_thumbnails_single_shot(ai_client, samples)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # RUN ALL 3 IN PARALLEL
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            futures = {}
            if has_titles:
                futures["title"] = loop.run_in_executor(executor, analyze_titles)
            if has_descriptions:
                futures["description"] = loop.run_in_executor(executor, analyze_descriptions)
            if has_thumbnails:
                futures["thumbnail"] = loop.run_in_executor(executor, analyze_thumbnails)

            # Gather all futures as asyncio tasks
            all_tasks = list(futures.items())
            pending_futures = {key: future for key, future in all_tasks}
            completed_keys = set()

            # Drain progress queue and await futures concurrently
            while pending_futures:
                # Drain all queued progress events
                while not progress_queue.empty():
                    try:
                        prog = progress_queue.get_nowait()
                        yield send_event("progress", prog)
                    except Exception:
                        break

                # Check which futures are done
                done_keys = []
                for key, future in pending_futures.items():
                    if future.done():
                        done_keys.append(key)
                        try:
                            result = future.result()
                            results[f"{key}_style"] = result
                            completed_keys.add(key)
                            label_map = {"title": "tiÃªu Ä‘á»", "description": "mÃ´ táº£", "thumbnail": "thumbnail"}
                            yield send_event("progress", {
                                "step": f"{key}_done",
                                "percentage": int((len(completed_keys) / total_tasks) * 80) + 10,
                                "message": f"HoÃ n thÃ nh phÃ¢n tÃ­ch {label_map.get(key, key)} âœ“"
                            })
                        except Exception as e:
                            logger.error(f"[Metadata-Analysis] Error analyzing {key}: {e}")
                            results[f"{key}_style"] = {"error": str(e)}
                            completed_keys.add(key)

                for key in done_keys:
                    del pending_futures[key]

                if pending_futures:
                    await asyncio.sleep(0.3)

            # Final drain of any remaining progress events
            while not progress_queue.empty():
                try:
                    prog = progress_queue.get_nowait()
                    yield send_event("progress", prog)
                except Exception:
                    break

        yield send_event("progress", {"step": "done", "percentage": 100, "message": "HoÃ n thÃ nh phÃ¢n tÃ­ch toÃ n bá»™!"})
        yield send_event("result", {
            "success": True,
            "title_style": results["title_style"],
            "description_style": results["description_style"],
            "thumbnail_style": results["thumbnail_style"],
        })

    return StreamingResponse(
        stream_analysis(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SINGLE-SHOT FALLBACKS (when conversation is not supported)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _analyze_titles_single_shot(ai_client, samples: list) -> dict:
    """Fallback: Analyze all titles in a single prompt."""
    samples_text = "\n".join([f"{i+1}. {t}" for i, t in enumerate(samples)])
    prompt = f"""Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch tiÃªu Ä‘á» YouTube. PhÃ¢n tÃ­ch {len(samples)} tiÃªu Ä‘á» máº«u vÃ  Ä‘Ãºc káº¿t thÃ nh "PHONG CÃCH Äáº¶T TIÃŠU Äá»€".

CÃC TIÃŠU Äá»€ MáºªU:
{samples_text}

TRáº¢ Vá»€ JSON:
{{
  "hook_patterns": ["pattern1", "pattern2"],
  "dominant_hooks": ["curiosity_gap", "specificity"],
  "avg_length": 45,
  "syntax_patterns": {{"front_loading": true, "separator": "|", "capitalization": "title_case"}},
  "power_words": ["tá»«1", "tá»«2", "tá»«3"],
  "emotional_tone": "mÃ´ táº£ tÃ´ng cáº£m xÃºc",
  "title_formulas": ["cÃ´ng thá»©c 1", "cÃ´ng thá»©c 2"],
  "style_summary": "TÃ³m táº¯t 2-3 cÃ¢u vá» phong cÃ¡ch Ä‘áº·t tiÃªu Ä‘á»"
}}"""
    result = ai_client.generate(prompt, temperature=0.3)
    parsed = _parse_metadata_json(result)
    return parsed if parsed else {"style_summary": result.strip(), "raw": True}


def _analyze_descriptions_single_shot(ai_client, samples: list) -> dict:
    """Fallback: Analyze all descriptions in a single prompt."""
    samples_text = "\n---\n".join([f"MáºªU {i+1}:\n{d[:2000]}" for i, d in enumerate(samples)])
    prompt = f"""Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch mÃ´ táº£ video YouTube. PhÃ¢n tÃ­ch {len(samples)} mÃ´ táº£ máº«u vÃ  Ä‘Ãºc káº¿t thÃ nh "PHONG CÃCH MÃ” Táº¢".

CÃC MÃ” Táº¢ MáºªU:
{samples_text}

TRáº¢ Vá»€ JSON:
{{
  "hook_style": "mÃ´ táº£ cÃ¡ch viáº¿t hook",
  "body_style": {{"avg_word_count": 200, "writing_approach": "tÃ³m táº¯t/ká»ƒ chuyá»‡n/liá»‡t kÃª", "uses_lsi_keywords": true}},
  "uses_timestamps": true,
  "timestamp_style": "mÃ´ táº£ phong cÃ¡ch timestamps náº¿u cÃ³",
  "cta_patterns": ["CTA 1", "CTA 2"],
  "cta_position": "top/middle/bottom/distributed",
  "hashtag_strategy": {{"avg_count": 3, "types": ["brand", "topic", "niche"]}},
  "brand_voice": {{"tone": "thÃ¢n máº­t/trang trá»ng/hÃ i hÆ°á»›c", "vocabulary": ["tá»« 1", "tá»« 2"], "addressing": "cÃ¡ch xÆ°ng hÃ´"}},
  "structure_template": "MÃ´ táº£ cáº¥u trÃºc máº«u cho description má»›i",
  "style_summary": "TÃ³m táº¯t 2-3 cÃ¢u vá» phong cÃ¡ch mÃ´ táº£"
}}"""
    result = ai_client.generate(prompt, temperature=0.3)
    parsed = _parse_metadata_json(result)
    return parsed if parsed else {"style_summary": result.strip(), "raw": True}


def _analyze_thumbnails_single_shot(ai_client, samples: list) -> dict:
    """Fallback: Analyze all thumbnails in a single prompt."""
    samples_text = "\n---\n".join([f"THUMBNAIL {i+1}:\n{t}" for i, t in enumerate(samples)])
    prompt = f"""Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch thumbnail YouTube. PhÃ¢n tÃ­ch {len(samples)} mÃ´ táº£ thumbnail máº«u vÃ  Ä‘Ãºc káº¿t thÃ nh "PHONG CÃCH THUMBNAIL".

CÃC MÃ” Táº¢ THUMBNAIL MáºªU:
{samples_text}

TRáº¢ Vá»€ JSON:
{{
  "composition": {{"layout": "rule_of_thirds/centered/asymmetric", "subject_position": "left/right/center", "visual_hierarchy": "mÃ´ táº£"}},
  "color_scheme": {{"dominant_colors": ["mÃ u 1", "mÃ u 2"], "contrast_style": "high/medium/low", "saturation": "hyper/normal/muted", "emotional_mapping": "mÃ´ táº£"}},
  "typography": {{"avg_word_count": 3, "font_style": "bold_sans_serif/custom/minimal", "effects": ["stroke", "shadow"]}},
  "emotional_signals": {{"uses_faces": true, "dominant_emotion": "shock/joy/curiosity/fear", "eye_contact": true, "exaggeration_level": "high/medium/low"}},
  "storytelling_techniques": ["before_after", "curiosity_gap", "arrows"],
  "overall_style": "professional/casual/cinematic",
  "text_strategy": "complementary/repetitive/curiosity_gap",
  "thumbnail_formula": "CÃ´ng thá»©c thumbnail Ä‘áº·c trÆ°ng",
  "style_summary": "TÃ³m táº¯t 2-3 cÃ¢u vá» phong cÃ¡ch thumbnail"
}}"""
    result = ai_client.generate(prompt, temperature=0.3)
    parsed = _parse_metadata_json(result)
    return parsed if parsed else {"style_summary": result.strip(), "raw": True}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SYNC REFERENCE ANALYSIS (Character / Style / Context) â€” SSE Streaming
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AnalyzeSyncReferencesRequest(BaseModel):
    character_text: str = ""
    style_text: str = ""
    context_text: str = ""
    character_images: List[str] = []  # base64 data URIs
    style_images: List[str] = []
    context_images: List[str] = []
    model: Optional[str] = None
    output_language: str = "vi"


@router.post("/analyze-sync-references-stream")
async def analyze_sync_references_stream(request: AnalyzeSyncReferencesRequest):
    """
    SSE streaming endpoint to analyze sync reference inputs (character, style, context).
    For each enabled sync type, uses AI (with Vision for images) to produce
    a refined, structured description optimized for downstream prompt generation.
    """
    has_character = bool(request.character_text.strip()) or len(request.character_images) > 0
    has_style = bool(request.style_text.strip()) or len(request.style_images) > 0
    has_context = bool(request.context_text.strip()) or len(request.context_images) > 0

    if not has_character and not has_style and not has_context:
        raise HTTPException(status_code=400, detail="Cáº§n Ã­t nháº¥t 1 loáº¡i sync reference")

    ai_client = get_configured_ai_client(request.model)
    lang_map = {"en": "English", "vi": "Vietnamese", "ja": "Japanese", "ko": "Korean", "zh": "Chinese"}
    lang_name = lang_map.get(request.output_language, request.output_language)

    async def stream_analysis():
        def send_event(event_type: str, data: dict) -> str:
            return f"event: {event_type}\ndata: {json_module.dumps(data, ensure_ascii=False)}\n\n"

        results = {"sync_character_analysis": None, "sync_style_analysis": None, "sync_context_analysis": None}
        total = sum([has_character, has_style, has_context])
        done_count = 0

        yield send_event("progress", {"step": "sync_init", "percentage": 0, "message": f"Báº¯t Ä‘áº§u phÃ¢n tÃ­ch {total} sync reference..."})

        loop = asyncio.get_event_loop()

        # â”€â”€ CHARACTER â”€â”€
        if has_character:
            yield send_event("progress", {"step": "sync_character", "percentage": 5, "message": "Äang phÃ¢n tÃ­ch nhÃ¢n váº­t..."})
            try:
                prompt = f"""You are a visual character description expert for AI video/image generation.
Analyze the character reference below and produce a DETAILED character description optimized for AI prompt generation.

USER INPUT:
{f'Text: {request.character_text}' if request.character_text.strip() else '(No text)'}
{f'{len(request.character_images)} reference image(s)' if request.character_images else '(No images)'}

OUTPUT a detailed character description in {lang_name} covering: physical appearance, face details, hair, clothing, expression/pose.
Format as a single cohesive paragraph (3-5 sentences). Output ONLY the description."""

                if request.character_images:
                    result = await loop.run_in_executor(None, lambda: ai_client.generate_with_images(prompt, request.character_images, temperature=0.3, max_tokens=1000))
                else:
                    result = await loop.run_in_executor(None, lambda: ai_client.generate(prompt, temperature=0.3))
                results["sync_character_analysis"] = result.strip() if result else None
                done_count += 1
                yield send_event("progress", {"step": "sync_character_done", "percentage": int((done_count / total) * 90) + 5, "message": "HoÃ n táº¥t phÃ¢n tÃ­ch nhÃ¢n váº­t"})
            except Exception as e:
                logger.error(f"[SyncAnalysis] Character error: {e}")
                done_count += 1
                yield send_event("progress", {"step": "sync_character_done", "percentage": int((done_count / total) * 90) + 5, "message": f"Lá»—i: {str(e)[:50]}"})

        # â”€â”€ STYLE â”€â”€
        if has_style:
            yield send_event("progress", {"step": "sync_style", "percentage": int((done_count / total) * 90) + 5, "message": "Äang phÃ¢n tÃ­ch phong cÃ¡ch..."})
            try:
                prompt = f"""You are a visual style analysis expert for AI video/image generation.
Analyze the style reference below and produce a DETAILED visual style specification optimized for AI prompt generation.

USER INPUT:
{f'Text: {request.style_text}' if request.style_text.strip() else '(No text)'}
{f'{len(request.style_images)} reference image(s)' if request.style_images else '(No images)'}

OUTPUT a detailed style specification in {lang_name} covering: visual style, lighting, color palette, camera/lens, mood/atmosphere.
Format as a single cohesive paragraph (3-5 sentences). Output ONLY the description."""

                if request.style_images:
                    result = await loop.run_in_executor(None, lambda: ai_client.generate_with_images(prompt, request.style_images, temperature=0.3, max_tokens=1000))
                else:
                    result = await loop.run_in_executor(None, lambda: ai_client.generate(prompt, temperature=0.3))
                results["sync_style_analysis"] = result.strip() if result else None
                done_count += 1
                yield send_event("progress", {"step": "sync_style_done", "percentage": int((done_count / total) * 90) + 5, "message": "HoÃ n táº¥t phÃ¢n tÃ­ch phong cÃ¡ch"})
            except Exception as e:
                logger.error(f"[SyncAnalysis] Style error: {e}")
                done_count += 1
                yield send_event("progress", {"step": "sync_style_done", "percentage": int((done_count / total) * 90) + 5, "message": f"Lá»—i: {str(e)[:50]}"})

        # â”€â”€ CONTEXT â”€â”€
        if has_context:
            yield send_event("progress", {"step": "sync_context", "percentage": int((done_count / total) * 90) + 5, "message": "Äang phÃ¢n tÃ­ch bá»‘i cáº£nh..."})
            try:
                prompt = f"""You are a scene/environment description expert for AI video/image generation.
Analyze the context/environment reference below and produce a DETAILED environment description optimized for AI prompt generation.

USER INPUT:
{f'Text: {request.context_text}' if request.context_text.strip() else '(No text)'}
{f'{len(request.context_images)} reference image(s)' if request.context_images else '(No images)'}

OUTPUT a detailed environment description in {lang_name} covering: location type, architecture/landscape, props/details, lighting/time, atmosphere.
Format as a single cohesive paragraph (3-5 sentences). Output ONLY the description."""

                if request.context_images:
                    result = await loop.run_in_executor(None, lambda: ai_client.generate_with_images(prompt, request.context_images, temperature=0.3, max_tokens=1000))
                else:
                    result = await loop.run_in_executor(None, lambda: ai_client.generate(prompt, temperature=0.3))
                results["sync_context_analysis"] = result.strip() if result else None
                done_count += 1
                yield send_event("progress", {"step": "sync_context_done", "percentage": int((done_count / total) * 90) + 5, "message": "HoÃ n táº¥t phÃ¢n tÃ­ch bá»‘i cáº£nh"})
            except Exception as e:
                logger.error(f"[SyncAnalysis] Context error: {e}")
                done_count += 1
                yield send_event("progress", {"step": "sync_context_done", "percentage": int((done_count / total) * 90) + 5, "message": f"Lá»—i: {str(e)[:50]}"})

        yield send_event("result", {"success": True, **results})

    return StreamingResponse(
        stream_analysis(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    )
